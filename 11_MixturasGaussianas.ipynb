{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Por si alguien corre en python2\n",
    "from __future__ import division\n",
    "\n",
    "# Preparamos todo para correr\n",
    "import numpy as np\n",
    "from math import *\n",
    "from matplotlib import pylab as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mixturas Gaussianas\n",
    "<a id='mixtures'></a>\n",
    "\n",
    "Cuando estudiamos la distribución normal, vimos que uno de las ventajas de sus propiedades analíticas es que sirven como elementos de modelos más complejos. \n",
    "\n",
    "Una técnica para hacer esto es la mixtura de Gaussianas, que nos permite captar mucha más complejidad que una simple normal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Si están en el colab, primero corran este código\n",
    "!mkdir datasets\n",
    "!wget https://raw.githubusercontent.com/exord/UNSAM_IA/master/datasets/faithful.csv\n",
    "!mv faithful.csv datasets/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Estos datos contienen información sobre la erupciones de geisers en Old Faithful, en el parque [Yellowstone](https://www.nps.gov/yell/learn/photosmultimedia/webcams.htm).\n",
    "\n",
    "Los datos son la duración de las erupciones en segundos, y el tiempo entre una erupción y la siguiente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Leamos y miremos datos de Faithful\n",
    "import pandas as pd\n",
    "df = pd.read_csv('datasets/faithful.csv')\n",
    "\n",
    "# Vamos las primeras columnas\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vemos que hay una columna inutil, que nos vamos a sacar de encima y después veamos los datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trash = df.pop('Unnamed: 0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l0 = df.plot('eruptions', 'waiting', kind='scatter')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Claramente, no podremos ajustar esto con una normal.\n",
    "Calculemos los valores de máxima verosimilitud para $\\mu$ y $\\Sigma$ usando los datos y veamos qué tal da.\n",
    "\n",
    "Recordemos los resultados\n",
    "\n",
    "$$\n",
    "\\mathbf{\\mu}_\\mathrm{ML} = \\frac{1}{N}\\sum_{n=1}^{N}\\mathbf{x}_n \\\\\n",
    "$$\n",
    "$$\n",
    "\\mathbf{\\Sigma}_\\mathrm{ML} = \\frac{1}{N}\\sum_{n=1}^{N} (\\mathbf{x}_n - \\mathbf{\\mu}_\\mathrm{ML})(\\mathbf{x}_n - \\mathbf{\\mu}_\\mathrm{ML})^T\n",
    "$$\n",
    "\n",
    "Calculemos eso para los datos de Old Faithful"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu_ml = df.mean().to_numpy()\n",
    "\n",
    "dx = (df - mu_ml)\n",
    "sigma_ml = np.dot(dx.T, dx) / len(dx)\n",
    "\n",
    "print(u'$\\mu_ML$ = {}'.format(mu_ml))\n",
    "print(u'$\\Sigma_ML$ = {}'.format(sigma_ml))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Con estos valores, construyamos una binormal. Como no vamos a sacar valores al azar, usamos la implementación en <tt> scipy.stats</tt>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats as st\n",
    "p = st.multivariate_normal(mean=mu_ml, cov=sigma_ml)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora viene la parte divertida. Vamos a graficar los contornos de la binormal sobre los datos de Old Faithful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Primero construyo una grilla de valores\n",
    "x, y = np.mgrid[1.0:6.0:0.1, 40:100:1]\n",
    "pos = np.dstack((x, y))\n",
    "\n",
    "# Prepara figure\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "# Vuelve a graficar datos grafica contornos (elegimos usar 4 contornos)\n",
    "df.plot('eruptions', 'waiting', kind='scatter', ax=ax)\n",
    "l0 = ax.contour(x, y, p.pdf(pos), 4, colors='0.5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Claramente el modelo de una Gaussiana no es bueno. El modelo no solo no recoje el comportamiento bimodal de los datos, sino que además contiene mucha densidad en el centro entre ambos grupos de puntos.\n",
    "\n",
    "Podemos calcular la verosimulitud de estos puntos. Suponiendo que son independientes:\n",
    "\n",
    "$$\n",
    "p(D | \\mu_\\mathrm{ML}, \\Sigma_\\mathrm{ML}) = \\prod_{n=1}^N \\mathcal{N(\\mathbf{x}_n | \\mu_\\mathrm{ML}, \\Sigma_\\mathrm{ML}})\\;\\;.\n",
    "$$\n",
    "\n",
    "Evaluo entonces la densidad de la distribución en cada punto y multiplico. Para eso, usamos la práctica función <tt>apply<\\tt> de Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "densities = df.apply(p.pdf, axis=1)\n",
    "print('La verosimilitud de este modelo es {}'.format(np.prod(densities)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "¿Qué pasó? Las densidades no son cero. Veamos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "densities.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "¿Alguna idea?\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nos encontramos con un problema típico cuando tratamos con muchos datos. La verosimilitud de cada uno de ellos es baja, y al multiplicarlos, llegamos por debajo del número más pequeño que puede representar la computadora. Este problema se hará más agudo cuantos más datos tengamos y cuanto mayor sea la dimensionalidad del espacio.\n",
    "\n",
    "La solución es trabajar con el logaritmo de la verosimilitud.\n",
    "\n",
    "En ese caso:\n",
    "\n",
    "$$\n",
    "\\ln p(D | \\mu_\\mathrm{ML}, \\Sigma_\\mathrm{ML}) = \\sum_{n=1}^N \\ln \\mathcal{N(\\mathbf{x}_n | \\mu_\\mathrm{ML}, \\Sigma_\\mathrm{ML}}) = -\\frac{1}{2}\\sum_{n=1}^{n} \\left[\\ln(|\\mathbf{\\Sigma}|) + \\ln(2\\pi) + (\\mathbf{x}_n - \\mathbf{\\mu}_\\mathrm{ML})^T \\mathbf{\\Sigma}^{-1}(\\mathbf{x}_n - \\mathbf{\\mu}_\\mathrm{ML})\\right]\\;\\;.\n",
    "$$\n",
    "\n",
    "Por suerte esto está implementado en <tt>scipy.stats</tt>.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logdensities = df.apply(p.logpdf, axis=1)\n",
    "print(logdensities.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ahora solo hay que sumar\n",
    "lnlike = np.sum(logdensities)\n",
    "print(lnlike)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fíjense que podríamos haber intentado directamente hacer el log de las densidades que ya teníamos; pero miren qué pasa\n",
    "lnlike2 = np.sum(np.log(densities))\n",
    "print(lnlike2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### Presentación del modelo\n",
    "\n",
    "Este ejemplo muestra una de las limitaciones de un modelo normal: solo puede captar distribuciones con un único modo. Una forma de sobreponerse a este problema es usar un modelo que consista en una superposición de $K$ distribuciones normales.\n",
    "\n",
    "$$\n",
    "p(\\mathbf{x}) = \\sum_{k=1}^K \\pi_k{\\mathcal{N}(\\mathbf{x} \\:|\\: \\boldsymbol{\\mu}_k, \\boldsymbol{\\Sigma}_k)}\\;\\;,\n",
    "$$\n",
    "donde cada densidad $\\mathcal{N}(\\mathbf{x} \\:|\\: \\boldsymbol{\\mu}_k, \\boldsymbol{\\Sigma}_k)$ se conoce como una *componente* de la mixtura, y tiene su propio valor medio y matriz de covarianza. Los $\\pi_k$ son los *coeficientes* de mezcla."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Los coeficientes como probabilidades__\n",
    "\n",
    "Integrando a ambos lados de la igualdad sobre todos los valore de $\\mathbf{x}$ y viendo que tanto $p(\\mathbf{x})$ como las componentes normales son funciones de distribución de probabilidad, llegamos a que\n",
    "\n",
    "$$\n",
    "\\sum_{k=1}^K \\pi_k = 1\\;\\;.\n",
    "$$\n",
    "\n",
    "Además, como $p(\\mathbf{x})$ tienen que ser positiva, encontramos que una forma de garantizar eso de manera general es definir que los coeficientes sean positivos. Por lo tanto, $0 \\leq \\pi_k \\leq 1$, y los coeficientes satisfacen los requerimientos para ser probabilidades.\n",
    "\n",
    "Podemos, entonces, asignar una función de distribución de masa, $p(k)$, que describe la probabilidad (prior) de que se elija la compoenente $k$, y $p(k) = \\pi_k$. Entonces, distribución sobre $\\mathbf{x}$ queda:\n",
    "\n",
    "$$\n",
    "p(\\mathbf{x}) = \\sum_{k=1}^K p(\\mathbf{x}, k) = \\sum_{k=1}^K p(k) p(\\mathbf{x} | k)\\;\\;,\n",
    "$$\n",
    "\n",
    "donde la probabilidad condicionada $p(\\mathbf{x} | k) = \\mathcal{N}(\\mathbf{x} \\:|\\: \\boldsymbol{\\mu}_k, \\boldsymbol{\\Sigma}_k)$.\n",
    "\n",
    "En otras palabras, visto de esta manera, para muestrear el modelo de mixtura puede pensarse como una cadena, donde uno decide primero a qué modo pertence un punto y luego muestrea ese modo. Esto se llama muestreo ancestral. Veámoslo en acción."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Supongamos una distribución unidimensional de dos modos, con los siguientes parámetros\n",
    "mu1 = [0.0,]\n",
    "sigma1 = [2.0,]\n",
    "mu2 = [5.0,]\n",
    "sigma2 = [1.5,]\n",
    "\n",
    "# Generemos las dos distris\n",
    "n1 = st.norm(mu1, sigma1)\n",
    "n2 = st.norm(mu2, sigma2)\n",
    "\n",
    "# Ahora pensemos que tenemos una distribución prior p(k) con las siguientes elementos\n",
    "p1 = 0.65\n",
    "p2 = 0.35\n",
    "\n",
    "# Veamos como podemos generar, fácilmente, muestras de la mixtura:\n",
    "# p1 * N1 + p2 * N2\n",
    "# Primero muestreo p(k)\n",
    "N = 5000\n",
    "k = np.where(np.random.rand(N) < p1, 1, 2)\n",
    "\n",
    "# Y ahora lo combino con las mixturas\n",
    "\n",
    "# Modo 1 (estricto)\n",
    "# x1 = n1.rvs(sum(k==1), 1)\n",
    "# x2 = n2.rvs(sum(k==2), 1)\n",
    "\n",
    "# x = np.hstack([x1, x2])\n",
    "\n",
    "# Modo 2 (generando de más)\n",
    "x1 = n1.rvs(N, 1)\n",
    "x2 = n2.rvs(N, 1)\n",
    "\n",
    "x = np.where(k==1, x1, x2).reshape((-1, 1))\n",
    "\n",
    "h = plt.hist(x, 50, histtype='step', density=True)\n",
    "\n",
    "# Combinemos esto con la pdf del modelo\n",
    "xx = np.linspace(x.min(), x.max(), 400)\n",
    "plt.plot(xx, p1 * n1.pdf(xx) + p2 * n2.pdf(xx), 'r-')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obviamente, como sabemos de dónde viene cada muestra, las podemos plotear por separado.\n",
    "h = plt.hist(x, h[1], histtype='step', lw=5, color='k', alpha=0.2)\n",
    "h1 = plt.hist(x[k==1], h[1], label='Componente 1', histtype='step')\n",
    "h2 = plt.hist(x[k==2], h[1], label='Componente 2', histtype='step')\n",
    "plt.legend(loc=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pero mantengamos en mente que en los problemas que vamos a resolver, contaremos con la distribución azul de la celda anterior y sin información sobre el origen de cada muestra. Es como si hubiéramos perdido el arreglo $\\mathbf{k}$. Al conjunto de datos $\\{\\mathbf{x}, \\mathbf{k}\\}$ se lo llama dataset completo, y a $\\mathbf{x}$ se lo conoce como dataset incompleto. \n",
    "\n",
    "Claro que nunca estuvo completo, en principio...."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Responsabilidades__\n",
    "\n",
    "Un rol crucial lo van a jugar las posteriores de la probabilidad de cada modo $p(k \\:|\\: \\mathbf{x})$, que se conocen como *responsabilidades* (¡cuánta seriedad!)\n",
    "\n",
    "$$\n",
    "\\gamma_k(\\mathbf{x}) \\equiv p(k \\:|\\: \\mathbf{x})\\;\\;.\n",
    "$$\n",
    "\n",
    "Como es de esperar, podemos calcular la punta que tienen las responsabilidades a partir del teorema de Bayes:\n",
    "\n",
    "$$\n",
    "\\begin{eqnarray}\n",
    "\\gamma_k(\\mathbf{x}) &=& \\frac{p(k) p(\\mathbf{x}\\:|\\:k)}{\\sum_i p(i) p(\\mathbf{x}\\:|\\:i)}\\\\\n",
    "   &=&\\frac{\\pi_k{\\mathcal{N}(\\mathbf{x} \\:|\\: \\boldsymbol{\\mu}_k, \\boldsymbol{\\Sigma}_k)}}{\\sum_i{\\pi_i{\\mathcal{N}(\\mathbf{x} \\:|\\: \\boldsymbol{\\mu}_i, \\boldsymbol{\\Sigma}_i)}}}\\;\\;,\n",
    "\\end{eqnarray}\n",
    "$$\n",
    "donde usamos la definición de arriba\n",
    "$$\n",
    "p(\\mathbf{x}) = \\sum_{k=1}^K \\pi_k{\\mathcal{N}(\\mathbf{x} \\:|\\: \\boldsymbol{\\mu}_k, \\boldsymbol{\\Sigma}_k)}\\;\\;.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Verosimilitud__ (o Donde arrancan los líos)\n",
    "\n",
    "Naturalmente, necesitaremos poder escribir la verosimilitud para un modelo de este tipo. Como vimos arriba, el problema con las exponenciales de la normal, es que muchas veces generan problemas numéricos.\n",
    "\n",
    "Vamos entonces a escribir el logaritmo de la verosimilitud, como ya hicimos un montón de veces.\n",
    "\n",
    "$$\n",
    "\\ln p(\\mathbf{X} \\:|\\: \\boldsymbol{\\mu}, \\boldsymbol{\\Sigma}, \\boldsymbol{\\pi}) = \\sum_{n=1}^N \\ln \\left\\{\\sum_{k=1}^K \\pi_k{\\mathcal{N}(\\mathbf{x}_n \\:|\\: \\boldsymbol{\\mu}_k, \\boldsymbol{\\Sigma}_k)}\\right\\}\\;\\;,\n",
    "$$\n",
    "donde estamos usando esta notación: $\\boldsymbol{\\mu} \\equiv \\left\\{\\boldsymbol{\\mu}_1, \\ldots, \\boldsymbol{\\mu}_K\\right\\}$, y lo mismo para $\\boldsymbol{\\Sigma}$ y para $\\boldsymbol{\\pi}$. Y como siempre $\\mathbf{X} = \\left\\{\\mathbf{x}_1, \\ldots, \\mathbf{x}_N\\right\\}$.\n",
    "\n",
    "Vemos que el logaritmo actúa sobre la sumatoria y no sobre cada normal de manera individual. Esto hace que sea complicado obtener una forma cerrada para los parámetros que maximizan la verosimilitud. Tenemos que usar un procedimiento iterativo, conocido como el algoritmo de maximización de la expectación, o *expectation maximisation* (EM).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Variables latentes / ocultas__\n",
    "\n",
    "Para presentar de forma correcta el EM, primero tenemos que introducir un conjunto de nuevas variables, llamadas latentes. Más que un deseo de rigurosidad insaciable, nos mueve el hecho de que los modelos que tienen variables latentes suelen ser capaces de captar mucha complejidad de forma sencilla, y resultan una herramienta fundamental a la hora de modelar datos de la vida real. (**NB**: diría que, más allá de cualquier algoritmo particular que se puedan llevar de la materia, el concepto de las variables latentes y el truco del kernel son las dos herramientas más poderosas que presentamos; son como el martillo y el serrucho de un constructor de modelos). Vamos a ver enseguida como, agrandar el problema y pensar en una distribución conjunta facilita la resolución del problema condicional, $p(\\mathbf{x} \\:|\\: k)$, que presentamos más arriba.\n",
    "\n",
    "Vamos a introducir una variable latente binaria, $K$-dimensional, $z$, con una representación 1-de-K. Es decir, todos los elementos son 0, excepto por el elemento $k$ que es uno y que indica a qué modo pertenece una determinada observación. Claramente, se verifica que $\\sum_k z_k = 1$ (la suma de los elementos de la variable $z$ es uno).\n",
    "\n",
    "Entonces, la relación con los coeficientes anteriores es:\n",
    "\n",
    "$$\n",
    "p(z_k = 1) = \\pi_k\\;\\;,\n",
    "$$\n",
    "y podemos escribir la distribución del _vector_ $\\mathbf{z}$ como\n",
    "\n",
    "$$\n",
    "p(\\mathbf{z}) = \\prod_{k=1}^K \\pi_k^{z_k}\\;\\;.\n",
    "$$\n",
    "\n",
    "Además, la condicional\n",
    "\n",
    "$$\n",
    "p(\\mathbf{x} \\:|\\: z_k = 1) = \\mathcal{N}(\\mathbf{x} \\:|\\: \\boldsymbol{\\mu}_k, \\boldsymbol{\\Sigma}_k)\n",
    "$$\n",
    "también puede escribirse para el vector $\\mathbf{z}$ de la misa manera\n",
    "$$\n",
    "p(\\mathbf{x} \\:|\\: \\mathbf{z}) = \\prod_{k=1}^K \\mathcal{N}(\\mathbf{x} \\:|\\: \\boldsymbol{\\mu}_k, \\boldsymbol{\\Sigma}_k)^{z_k}\\;\\;.\n",
    "$$\n",
    "\n",
    "Bien, podemos entonces, plantear el problema original en términos de las distribuciones conjuntas, condicionales y marginales, $p(\\mathbf{x}, \\mathbf{z})$, $p(\\mathbf{x} \\:|\\: \\mathbf{z})$ y $p(\\mathbf{z})$, que, como sabemos desde la primera clase, se relacionan a través de la regla del producto:\n",
    "\n",
    "$$\n",
    "p(\\mathbf{x}, \\mathbf{z}) = p(\\mathbf{x} \\:|\\: \\mathbf{z}) p(\\mathbf{z})\\;\\;,\n",
    "$$\n",
    "y también podemos marginalizar sobre las variables latentes para recuperar el modelo anterior:\n",
    "\n",
    "$$\n",
    "p(\\mathbf{x}) = \\sum_\\mathbf{z} p(\\mathbf{x} \\:|\\: \\mathbf{z}) p(\\mathbf{z}) = \\sum_{k=1}^K \\pi_k{\\mathcal{N}(\\mathbf{x} \\:|\\: \\boldsymbol{\\mu}_k, \\boldsymbol{\\Sigma}_k)}\\;\\;.\n",
    "$$\n",
    "\n",
    "O sea, estoy hace media hora dándole vueltas a las ecuaciones para llegar a lo mismo. ¿O no? \n",
    "En principio, vean que para cada dato $n$ habrá una variable latente $\\mathbf{z}_n$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__El algoritmo EM__\n",
    "\n",
    "La gran ventaja de este planteo es que ahora podemos trabajar con la distribución conjunta $p(\\mathbf{x}, \\mathbf{z})$, que nos da pie para poner en marcha un algoritmo muy general, que sirva para todos los modelos que usen variables latentes. \n",
    "\n",
    "Acá vamos a presentarlo solo para las mixturas gaussianas. Pueden encontrar un tratamiento mucho más general en la Sect. 9.4 del Bishop.\n",
    "\n",
    "Vamos a definir dos matrices, con los datos incompletos $\\mathbf{X}$ y con las variables latentes $\\mathbf{Z}$, cuya fila $n$ corresponde al vector de variables latentes del dato $\\mathbf{x}_n$, $\\mathbf{z}_n$.\n",
    "Primero, volvamos a escribir el logaritmo de la verosimilitud:\n",
    "$$\n",
    "\\ln p(\\mathbf{X} \\:|\\: \\boldsymbol{\\mu}, \\boldsymbol{\\Sigma}, \\boldsymbol{\\pi}) = \\sum_{n=1}^N \\ln \\left\\{\\sum_{k=1}^K \\pi_k{\\mathcal{N}(\\mathbf{x}_n \\:|\\: \\boldsymbol{\\mu}_k, \\boldsymbol{\\Sigma}_k)}\\right\\}\\;\\;.\n",
    "$$\n",
    "\n",
    "Derivando con respecto a $\\boldsymbol{\\mu}_k$ e igualando a cero, tenemos:\n",
    "\n",
    "$$\n",
    "0 = \\sum_{n=1}^N \\gamma(z_{nk}) \\Sigma_k^{-1} (\\mathbf{x_n} - \\boldsymbol{\\mu}_k)\\;\\;.\n",
    "$$\n",
    "\n",
    "Multiplicando a ambos lados por $\\Sigma_k$, llegamos a:\n",
    "\n",
    "$$\n",
    "\\boldsymbol{\\mu}_k = \\frac{1}{N_k} \\sum_{n=1}^N {\\gamma(z_{nk}) \\mathbf{x}_n}\\;\\;,\n",
    "$$\n",
    "\n",
    "con\n",
    "\n",
    "$$\n",
    "N_k = \\sum_{n=1}^N {\\gamma(z_{nk})}\\;\\;,\n",
    "$$\n",
    "\n",
    "que es una especie de número efectivo de muestras provenientes del modo $k$.\n",
    "\n",
    "Lo mismo podemos hacer para la covarianza $\\Sigma_k$ y da\n",
    "\n",
    "$$\n",
    "\\boldsymbol{\\Sigma}_k = \\frac{1}{N_k} \\sum_{n=1}^N {\\gamma(z_{nk}) (\\mathbf{x}_n - \\boldsymbol{\\mu}_k)(\\mathbf{x}_n - \\boldsymbol{\\mu}_k)^T}\\;\\;,\n",
    "$$\n",
    "\n",
    "y para los coeficientes de mezcla, usando el constrain (via multiplicadores de Lagrange) de que su suma tienen que ser uno.\n",
    "\n",
    "$$\n",
    "\\pi_k = \\frac{N_k}{N}\\;\\;.\n",
    "$$\n",
    "\n",
    "Hay que ver que la expresión para las medias y la covarianza tiene la misma pinta que para el caso de una sola distribución gaussiana. Pero como aparecen las responsabilidades, la forma no es cerrada.\n",
    "\n",
    "Se vislumbra un proceso iterativo de la siguiente pinta:\n",
    "\n",
    "1. Se inicializan los parámetros $\\boldsymbol{\\mu}_k$, $\\boldsymbol{\\Sigma}_k$, y $\\pi_k$.\n",
    "\n",
    "2. Se calculan las responsabilidades para esos valores de los parámetros (paso E).\n",
    "\n",
    "3. Con las nuevas responsabilidades, se recalculan los parámetros que maximizan la verosimilitud (paso M). Como antes, podemos actualizar las medias antes de calcular el nuevo valor de las covarianzas.\n",
    "\n",
    "4. Volver a calcular el logaritmo de la verosimilitud.\n",
    "\n",
    "5. Evaluar algún criterio de detención, sea en LogL o en los parámetros."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a ver cómo funciona con nuestro set unidimensional. `sklearn` tiene implementado el algo EM en la clase `\n",
    "mixture.GaussianMixture`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.mixture import GaussianMixture\n",
    "\n",
    "# Veamos la docu\n",
    "GaussianMixture?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gm = GaussianMixture(2, means_init=np.array([0.0, 5.0]).reshape(2, 1), weights_init=[0.5, 0.5], tol=1e-8)\n",
    "gm.fit(x)\n",
    "\n",
    "print('Fitted parameters')\n",
    "print('Mixing coeff:', gm.weights_)\n",
    "print('Means:', gm.means_)\n",
    "print('widhts:', np.sqrt(gm.covariances_))\n",
    "\n",
    "# Supongamos una distribución unidimensional de dos modos, con los siguientes parámetros\n",
    "# mu1 = [0.0,]\n",
    "# sigma1 = [2.0,]\n",
    "# mu2 = [5.0,]\n",
    "# sigma2 = [1.5,]\n",
    "# p1 = 0.65\n",
    "# p2 = 0.35\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparemos con la densidad real\n",
    "plt.plot(xx, p1 * n1.pdf(xx) + p2 * n2.pdf(xx), 'r-', label='Truth', lw=5, alpha=0.4)\n",
    "plt.plot(xx, np.exp(gm.score_samples(xx.reshape(-1, 1))), label='ML fit', color='k')\n",
    "plt.legend(loc=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tp = gm.predict(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "**A trabajar!**\n",
    "\n",
    "Ahora a ustedes. Tomen el dataset de Old Faithful de acá arriba y ajusten un modelo de mixtura. Usen la clase de skelearn.\n",
    "\n",
    "Después copien el código que está arriba para plotear los contornos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gm = GaussianMixture(2, tol=1e-3)\n",
    "gm.fit(df)\n",
    "\n",
    "print('Fitted parameters')\n",
    "print('Mixing coeff:', gm.weights_)\n",
    "print('Means:', gm.means_)\n",
    "print('widhts:', np.sqrt(gm.covariances_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Primero construyo una grilla de valores\n",
    "x, y = np.mgrid[1.0:6.0:0.1, 40:100:1]\n",
    "pos = pos = np.dstack((x, y))\n",
    "\n",
    "# Compute mixture pdf\n",
    "\n",
    "# Usando sklearn\n",
    "X_ = np.c_[x.ravel(), y.ravel()]\n",
    "\n",
    "# Calculo la clase\n",
    "clase1d = gm.predict(X_)\n",
    "clase2d = clase1d.reshape(x.shape)\n",
    "\n",
    "# Calculo la densidad\n",
    "pdf = np.exp(gm.score_samples(X_))\n",
    "P = pdf.reshape(x.shape)\n",
    "\n",
    "# \"A mano\"\n",
    "# P = (gm.weights_[0] * st.multivariate_normal.pdf(pos, mean=gm.means_[0], cov=gm.covariances_[0]) +\n",
    "#        gm.weights_[1] * st.multivariate_normal.pdf(pos, mean=gm.means_[1], cov=gm.covariances_[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepara figure\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "# Vuelve a graficar datos y densidad\n",
    "l0 = ax.contourf(x, y, P, 256, alpha=1)\n",
    "df.plot('eruptions', 'waiting', kind='scatter', ax=ax, c='w', alpha=0.4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepara figure\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "# Vuelve a graficar datos y clases\n",
    "l0 = ax.contourf(x, y, clase2d, alpha=1)\n",
    "df.plot('eruptions', 'waiting', kind='scatter', ax=ax, c=gm.predict(df), alpha=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicción de clase para un nuevo dato y su responsabilidad.\n",
    "\n",
    "x_nuevo = np.array([3.0, 65.0]).reshape(1, 2)\n",
    "print(gm.predict(x_nuevo))\n",
    "print(gm.predict_proba(x_nuevo))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Marginalizamos uno de los features (la duración de la erupción).\n",
    "\n",
    "xx = np.linspace(40, 100, 200)\n",
    "\n",
    "scale1 = np.sqrt(gm.covariances_[0][1,1])\n",
    "scale2 = np.sqrt(gm.covariances_[1][1,1])\n",
    "\n",
    "pp = (gm.weights_[0] * st.norm.pdf(xx, loc=gm.means_[0][1], scale=scale1) + \n",
    "      gm.weights_[1] * st.norm.pdf(xx, loc=gm.means_[1][1], scale=scale2))\n",
    "\n",
    "plt.plot(xx, pp)\n",
    "plt.xlabel('waiting')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nos quedamos con una única componente\n",
    "k = 1\n",
    "\n",
    "# No vale la pena incluir el coeficiente, porque después hay que normalizar\n",
    "pp = st.multivariate_normal.pdf(pos, mean=gm.means_[k], cov=gm.covariances_[k])\n",
    "\n",
    "l0 = plt.contourf(x, y, pp, alpha=1)\n",
    "#plt.colorbar(l0)\n",
    "#df.plot('eruptions', 'waiting', kind='scatter', ax=ax, c='w', alpha=0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-means\n",
    "\n",
    "Es un algoritmo de clustering que funciona minimizando la distancia de los puntos a los centros de los clusters.\n",
    "\n",
    "Veamos su funcionamiento con los datos de Old Faithful"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "km = KMeans(n_clusters=2)\n",
    "km.fit(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Veamos los parámetros\n",
    "km.cluster_centers_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(df.eruptions, df.waiting, c=km.labels_)\n",
    "plt.plot(km.cluster_centers_[:, 0], km.cluster_centers_[:, 1], 'or', markersize=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
