{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "cmap = matplotlib.cm.get_cmap('Spectral')\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.datasets import make_moons, make_hastie_10_2\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import BaggingClassifier, RandomForestClassifier, AdaBoostClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Funciones útiles\n",
    "\n",
    "def print_results(clfrgr):\n",
    "    print('#####')\n",
    "    print('{}.'.format(type(clfrgr).__name__))\n",
    "                       \n",
    "    train_score = clfrgr.score(X_train, y_train)\n",
    "    test_score = clfrgr.score(X_test, y_test)\n",
    "    print('Train score: ', train_score)\n",
    "    print('Test score: ', test_score)\n",
    "\n",
    "def print_results_boost(clfrgr):\n",
    "    print('#####')\n",
    "    print('{} con {} estimadores de tipo {}, con max_depth={}.'.format(type(clfrgr).__name__,\n",
    "                                                                       clfrgr.n_estimators, \n",
    "                                                                       type(clfrgr.base_estimator).__name__,\n",
    "                                                                       clfrgr.base_estimator.max_depth))\n",
    "    train_score = clfrgr.score(X_train, y_train)\n",
    "    test_score = clfrgr.score(X_test, y_test)\n",
    "    print('Train score: ', train_score)\n",
    "    print('Test score: ', test_score)\n",
    "\n",
    "    return train_score, test_score\n",
    "\n",
    "def plot_decision_boundary(clf, X, y, axes=[-1.5, 2.5, -1, 1.5], alpha=0.5, contour=True):\n",
    "    x1s = np.linspace(axes[0], axes[1], 100)\n",
    "    x2s = np.linspace(axes[2], axes[3], 100)\n",
    "    x1, x2 = np.meshgrid(x1s, x2s)\n",
    "    X_new = np.c_[x1.ravel(), x2.ravel()]\n",
    "    y_pred = clf.predict(X_new).reshape(x1.shape)\n",
    "    custom_cmap = ListedColormap(['#fafab0','#9898ff','#a0faa0'])\n",
    "    plt.contourf(x1, x2, y_pred, alpha=0.3, cmap=custom_cmap)\n",
    "    if contour:\n",
    "        custom_cmap2 = ListedColormap(['#7d7d58','#4c4c7f','#507d50'])\n",
    "        plt.contour(x1, x2, y_pred, cmap=custom_cmap2, alpha=0.8)\n",
    "    ax = plt.gca()\n",
    "    \n",
    "    plt.plot(X[:, 0][y==0], X[:, 1][y==0], \"o\", color=cmap(0), alpha=alpha)\n",
    "    plt.plot(X[:, 0][y==1], X[:, 1][y==1], \"yo\", alpha=alpha)\n",
    "    plt.axis(axes)\n",
    "    plt.xlabel(r\"$x_1$\", fontsize=18)\n",
    "    plt.ylabel(r\"$x_2$\", fontsize=18, rotation=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = make_moons(5000, shuffle=True, noise=0.2)\n",
    "size_train = int(0.8 * len(X))\n",
    "X_train, X_test = X[:size_train], X[size_train:]\n",
    "y_train, y_test = y[:size_train], y[size_train:]\n",
    "\n",
    "plt.scatter(*X_train.T, c=y_train, marker='.')\n",
    "#plt.scatter(*X_test.T, c=y_test, marker='^')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = DecisionTreeClassifier(max_depth=12, random_state=107)\n",
    "dt = dt.fit(X_train, y_train)\n",
    "\n",
    "# svm_pipe = Pipeline(steps=[('scaler', StandardScaler()), \n",
    "#                            ('svc', SVC(kernel='poly'))])\n",
    "# svm_pipe = svm_pipe.fit(X_train, y_train)\n",
    "\n",
    "plot_decision_boundary(dt, X_train, y_train, contour=True, alpha=0.1)\n",
    "print_results(dt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import BaggingClassifier\n",
    "\n",
    "clf = BaggingClassifier(DecisionTreeClassifier(max_depth=12, random_state=107), n_estimators=500, n_jobs=-1, \n",
    "                        oob_score=False, max_samples=1000, bootstrap=True)\n",
    "clf = clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(clf.oob_decision_function_.shape)\n",
    "print(clf.oob_score_)\n",
    "print(clf.oob_decision_function_[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_decision_boundary(clf, X_train, y_train, alpha=0.1)\n",
    "print_results(clf)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extra trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import ExtraTreeClassifier\n",
    "\n",
    "dt = ExtraTreeClassifier(max_depth=12, random_state=107)\n",
    "dt = dt.fit(X_train, y_train)\n",
    "\n",
    "# svm_pipe = Pipeline(steps=[('scaler', StandardScaler()), \n",
    "#                            ('svc', SVC(kernel='poly'))])\n",
    "# svm_pipe = svm_pipe.fit(X_train, y_train)\n",
    "\n",
    "plot_decision_boundary(dt, X_train, y_train, contour=True, alpha=0.1)\n",
    "print_results(dt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = BaggingClassifier(ExtraTreeClassifier(max_depth=12, random_state=107), n_estimators=500, n_jobs=-1, \n",
    "                        oob_score=False, max_samples=1000, bootstrap=True)\n",
    "clf = clf.fit(X_train, y_train)\n",
    "\n",
    "plot_decision_boundary(clf, X_train, y_train, alpha=0.1)\n",
    "print_results(clf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bias-Variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = DecisionTreeClassifier(max_depth=1, random_state=107)\n",
    "dt = dt.fit(X_train, y_train)\n",
    "\n",
    "# svm_pipe = Pipeline(steps=[('scaler', StandardScaler()), \n",
    "#                            ('svc', SVC(kernel='poly'))])\n",
    "# svm_pipe = svm_pipe.fit(X_train, y_train)\n",
    "\n",
    "plot_decision_boundary(dt, X_train, y_train, contour=True, alpha=0.1)\n",
    "print_results(dt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = BaggingClassifier(DecisionTreeClassifier(max_depth=1, random_state=107), n_estimators=500, n_jobs=-1, \n",
    "                        oob_score=False, max_samples=1000, bootstrap=True)\n",
    "clf = clf.fit(X_train, y_train)\n",
    "\n",
    "plot_decision_boundary(clf, X_train, y_train, alpha=0.1)\n",
    "print_results(clf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para mostrar el poder de boosting, hagamos un nuevo dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = make_hastie_10_2(random_state=0)\n",
    "\n",
    "print(X.shape)\n",
    "\n",
    "X_train, X_test = X[2000:], X[:2000]\n",
    "y_train, y_test = y[2000:], y[:2000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Este dataset consiste en diez features normales estándares, con los siguientes labels: \n",
    "\n",
    "`y[i] = 1 if np.sum(X[i] ** 2) > 9.34 else -1`\n",
    "\n",
    "Es decir, es una hiperesfera adentro de una hiperesfera hueca."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "dd = pd.DataFrame.from_records(X_train)\n",
    "dd['target'] = y_train\n",
    "\n",
    "#ss.pairplot(dd.iloc[::20, :5], hue='target')\n",
    "#ss.pairplot(dd.iloc[::20], hue='target', vars=range(5), markers=',')\n",
    "sns.pairplot(dd.iloc[::20], hue='target', vars=range(4), markers='.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Veamos como funciona `AdaBoost` en un caso así. Combinemos DTs con profundidad máxima 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = AdaBoostClassifier(base_estimator=DecisionTreeClassifier(max_depth=2), n_estimators=100, learning_rate=1.0)\n",
    "\n",
    "clf.fit(X_train, y_train)\n",
    "print_results_boost(clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for nesti in [10, 20, 50, 75]:\n",
    "    clf.set_params(n_estimators=nesti)\n",
    "    clf.fit(X_train, y_train)\n",
    "    print_results_boost(clf)\n",
    "\n",
    "clf.set_params(n_estimators=100)\n",
    "clf.base_estimator.set_params(max_depth=1)\n",
    "clf.fit(X_train, y_train)\n",
    "print_results(clf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stacking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import RidgeCV, LassoCV\n",
    "from sklearn.svm import SVR\n",
    "estimators = [('ridge', RidgeCV()),\n",
    "              ('lasso', LassoCV(random_state=42)),\n",
    "              ('svr', SVR(C=1, gamma=1e-6))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.ensemble import StackingRegressor\n",
    "\n",
    "reg = StackingRegressor(estimators=estimators, final_estimator=GradientBoostingRegressor(random_state=42))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_boston\n",
    "X, y = load_boston(return_X_y=True)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Y se ajusta como siempre, con la magia de sklearn\n",
    "reg.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Y podemos obtener el output de cada uno con el método `transform`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
