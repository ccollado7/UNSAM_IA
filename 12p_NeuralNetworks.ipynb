{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nC18EFXdSqmd"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "#The stars\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "DuLqMAEqS5fa",
    "outputId": "00ce0bd4-c74b-48fe-a10a-606099f07451"
   },
   "outputs": [],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "a-BToGrRTCK1",
    "outputId": "8a4c1beb-29b5-459a-fcbd-5bac2c721f7b"
   },
   "outputs": [],
   "source": [
    "keras.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "id": "GTr73m2gTDu2",
    "outputId": "11e46b19-d5d6-43db-cc10-f68036941788"
   },
   "outputs": [],
   "source": [
    "tf.config.list_physical_devices()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aoNOblmHX1XY"
   },
   "source": [
    "# Datos: Clasificacion de ropa (Fashion_MNIST)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0PMm5jBHq_7a"
   },
   "source": [
    "Asi como sklearn trae utilidades para cargar datasets estandar, keras también trae. En general, keras puede aceptar datasets en forma de Numpy Arrays (como sklearn), pero también trae una clase Dataset que esta optimizada para cargar datos (incluso si son mas grandes que la memoria RAM del equipo)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 173
    },
    "colab_type": "code",
    "id": "xiVqgSJhTeXp",
    "outputId": "dfaf44f5-1db9-4ae0-b0b0-ea65d5ff8d49"
   },
   "outputs": [],
   "source": [
    "fashion_mnist = keras.datasets.fashion_mnist\n",
    "(X_train_full, y_train_full), (X_test, y_test) = fashion_mnist.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uIus71mjtqxf"
   },
   "source": [
    "En Datasets grandes, CrossValidation puede ser muy costoso, por lo cual se recomienda separar un conjunton de validación aparte del training. Esto hacemos a continuación.\n",
    "\n",
    "También normalizamos los píxeles (que van de 0 a 255) para que estén entre 0 y 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7cB3KYyaUnzP"
   },
   "outputs": [],
   "source": [
    "#Separo en entrenamiento y validacion, y normalizo los pixeles\n",
    "X_valid, X_train = X_train_full[:5000] / 255., X_train_full[5000:] / 255.\n",
    "y_valid, y_train = y_train_full[:5000], y_train_full[5000:]\n",
    "X_test = X_test / 255."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 299
    },
    "colab_type": "code",
    "id": "I16qmEbDVnsa",
    "outputId": "ac2ff7f3-cc61-46f8-dce4-f318540df518"
   },
   "outputs": [],
   "source": [
    "print(X_train_full.shape)\n",
    "\n",
    "plt.imshow(X_train[0], cmap='binary', interpolation='bicubic')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2nY8YQ_0uEPg"
   },
   "source": [
    "Los targets son numericos, del 0 al 9. Podemos guardar las etiquetas asi nos es mas facil analizar que tan bien o mal funciona nuestro modelo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 308
    },
    "colab_type": "code",
    "id": "xNkEq7xEU1YD",
    "outputId": "057d5977-5bc4-47a3-b770-ed4800c275ff"
   },
   "outputs": [],
   "source": [
    "class_names = [\"Remera/top\", \"Pantalones\", \"Pullover\", \"Vestido\", \"Abrigo\",\n",
    "               \"Sandalias\", \"Camisa\", \"Zapatillas\", \"Bolso\", \"Bota\"]\n",
    "\n",
    "n_rows = 4\n",
    "n_cols = 10\n",
    "plt.figure(figsize=(n_cols * 1.2, n_rows * 1.2))\n",
    "for row in range(n_rows):\n",
    "    for col in range(n_cols):\n",
    "        index = n_cols * row + col\n",
    "        plt.subplot(n_rows, n_cols, index + 1)\n",
    "        plt.imshow(X_train[index], cmap=\"binary\", interpolation=\"nearest\")\n",
    "        plt.axis('off')\n",
    "        plt.title(class_names[y_train[index]], fontsize=12)\n",
    "plt.subplots_adjust(wspace=0.2, hspace=0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "P_staGAxY9yH"
   },
   "source": [
    "# Definir un modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OsfK9opSuPfg"
   },
   "source": [
    "Hay tres formas de definir modelos:\n",
    "* Una es utilizando la clase de ``keras.models.Sequential``, que es apta para modelos donde solamente Layers adyacentes se encuentran conectados entre sí. \n",
    "* Otra es API funcional, que provee mucha mas flexibilidad, ya que se puede construir cualquier clase de red dirigida acíclica. \n",
    "* La ultima forma, es con SubClassing, es decir definiendo nuestras propias clases que hereden de las definidas en Keras, lo cual nos permite extender Keras para nuestras necesidades específicas, aunque no es recomendable a menos que realmente sepamos lo que estamos haciendo.\n",
    "\n",
    "Si bien ahora quizás les sea mas sencillo utilizar la API secuencial, aprender a utilizar la API funcional es casi igual de fácil y provee mas flexibilidad, asi que es la forma que recomiendo de aprender a usar."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "36PryIS5X_Oc"
   },
   "source": [
    "## API Secuencial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uZXmZK8yXF4u"
   },
   "source": [
    "### Modo 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "b3lSxEA-vSsr"
   },
   "source": [
    "Podemos crear un modelo, e ir añadiendo Layers sobre la marcha."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Mvgn-wSlYV89"
   },
   "outputs": [],
   "source": [
    "keras.backend.clear_session()\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Cnwg1ZjvXLZE"
   },
   "outputs": [],
   "source": [
    "#Creo un modelo secuencial\n",
    "model = keras.models.Sequential()\n",
    "#Y le voy agregando layers\n",
    "model.add(keras.layers.Flatten(input_shape=[28, 28])) #matriz->vector (como np.ravel)\n",
    "model.add(keras.layers.Dense(300, activation=\"relu\")) #hidden 1\n",
    "model.add(keras.layers.Dense(100, activation=\"relu\")) #hidden 2\n",
    "model.add(keras.layers.Dense(10, activation=\"softmax\")) #output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jojgQMNjYFQu"
   },
   "source": [
    "### Modo 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eb17UrHJvTv8"
   },
   "source": [
    "También podemos directamente pasar la lista de Layers al constructor del Modelo ``Sequential``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "63vXIjKCYTOm"
   },
   "outputs": [],
   "source": [
    "keras.backend.clear_session()\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "C_2k6HdfXlr5"
   },
   "outputs": [],
   "source": [
    "#Sino directamente le paso una lista al crearlo:\n",
    "\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=[28, 28]),\n",
    "    keras.layers.Dense(300, activation=\"relu\"),\n",
    "    keras.layers.Dense(100, activation=\"relu\"),\n",
    "    keras.layers.Dense(10, activation=\"softmax\")\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TBYZ4zFYYHDG"
   },
   "source": [
    "## API Funcional"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0gWy2OctvbZF"
   },
   "source": [
    "La API funcional funciona definiendo Layers y utilizandolo como si fueran \"funciones\" que se aplican a los outputs de otros Layers. Keras irá rastreando estas conecciones, y luego generará un modelo entre el Layer de Inputs, y el de Outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_BdBiQp1YXiJ"
   },
   "outputs": [],
   "source": [
    "keras.backend.clear_session()\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "F-1uPqZmYIeh"
   },
   "outputs": [],
   "source": [
    "input_ = keras.layers.Input(shape=[28, 28]) #En este caso es necesario definir el Layer de Inputs\n",
    "flatten = keras.layers.Flatten()(input_)\n",
    "hidden1 = keras.layers.Dense(300, activation=\"relu\")(flatten)\n",
    "hidden2 = keras.layers.Dense(100, activation=\"relu\")(hidden1)\n",
    "output = keras.layers.Dense(10, activation=\"softmax\")(hidden2)\n",
    "model = keras.models.Model(inputs=[input_], outputs=[output])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9OwGQ8puZJmI"
   },
   "source": [
    "# Compilar y visualizar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Y1n6UL-ovs4G"
   },
   "source": [
    "Una vez que hemos definido nuestro modelo, es necesario que lo \"compilemos\", entonces Keras creará el gráfico computacional en TensorFlow de acuerdo a como lo hemos definido."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OVncsjmrZMbz"
   },
   "outputs": [],
   "source": [
    "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "              optimizer=\"sgd\",\n",
    "              metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zARoy-X0v66Q"
   },
   "source": [
    "Podemos acceder a los diferentes Layers, mediante el atributo ``.layers``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "id": "AaYiz7qVZqSw",
    "outputId": "01674a4a-7186-4ad9-eb61-0cd631a2dec2"
   },
   "outputs": [],
   "source": [
    "model.layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lYy-e7pzwBHz"
   },
   "source": [
    "Y También podemos imprimir un resumen de nuestro modelo, asi como un lindo grafico de el:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 289
    },
    "colab_type": "code",
    "id": "0LXinQM0ZquT",
    "outputId": "c9e379e0-7026-4193-f273-7dd84cee694c"
   },
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 533
    },
    "colab_type": "code",
    "id": "_HugLm6sZvmx",
    "outputId": "537ac4fb-8984-48f1-9b4e-03a1efa99bc0"
   },
   "outputs": [],
   "source": [
    "keras.utils.plot_model(model, \"my_fashion_mnist_model.png\", show_shapes=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QcFeksoHwHbQ"
   },
   "source": [
    "Cada layer tiene atributos a los que podemos acceder, como el nombre, y métodos que nos devuelven, por ejemplo, los pesos.\n",
    "\n",
    "Vemos que por default inicializa los pesos de forma aleatoria entre 0 y 1, y los bias a cero. Esto se puede cambiar pasando un ``initializer=``al compilar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 901
    },
    "colab_type": "code",
    "id": "1Js3Y9CgZ17S",
    "outputId": "015abc71-3978-46cd-d80d-8dcd9689b17b"
   },
   "outputs": [],
   "source": [
    "hidden1 = model.layers[1]\n",
    "print(hidden1.name)\n",
    "weights, biases = hidden1.get_weights()\n",
    "print(weights.shape)\n",
    "print(weights[0], biases[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aRf0_MB6ZdBX"
   },
   "source": [
    "# Entrenar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jRSYAHruwcjM"
   },
   "source": [
    "Entrenar es tan facil como hacer un ``.fit``, donde podemos especificar muchas cosas como épocas, datos de validación (que evaluará al final de cada época), métricas, etc.\n",
    "\n",
    "El método devuelve un objeto de Historia, con toda la información del entrenamiento (en forma de diccionarios), que podremos utilizar para analizar el modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 717
    },
    "colab_type": "code",
    "id": "rL9AKbgfZeQe",
    "outputId": "fdfe6454-d036-4855-95a8-f14584d11ed2"
   },
   "outputs": [],
   "source": [
    "history = model.fit(X_train, y_train, epochs=20,\n",
    "                    validation_data=(X_valid, y_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ubNbhHnEwwCq"
   },
   "source": [
    "Examinemos el diccionario de parámetros del fit:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "IITIPuw9aATd",
    "outputId": "8adf9a94-93a5-45fd-c8e0-e4a9d5180253"
   },
   "outputs": [],
   "source": [
    "history.params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "rgFx-tzNaGxG",
    "outputId": "291bdd71-8a6a-471e-9d9c-848741885fb5"
   },
   "outputs": [],
   "source": [
    "history.history.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_Tg1GkrJw11Y"
   },
   "source": [
    "Podemos usar el atributo \"history\" que nos da un diccionario, para plotear las métricas que usamos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 324
    },
    "colab_type": "code",
    "id": "5ksDH-nWaLfX",
    "outputId": "8644fb44-ac33-4aef-cb8a-719d4bc90bd1"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "pd.DataFrame(history.history).plot(figsize=(8, 5))\n",
    "plt.grid(True)\n",
    "plt.gca().set_ylim(0, 1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6zMsHUuRZgBj"
   },
   "source": [
    "# Evaluar y predecir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "d0RQHI_GxEw0"
   },
   "source": [
    "Evaluar en un conjunto de Test, o hacer predicciones, es sumamente fácil:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "FYcvFqohZhVv",
    "outputId": "0fdf5824-d365-4039-ccae-8c72a843d2d0"
   },
   "outputs": [],
   "source": [
    "model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "id": "6npHqXo7aSUs",
    "outputId": "017d6c3e-746e-4a15-d854-63218cc4d578"
   },
   "outputs": [],
   "source": [
    "X_new = X_test[:3]\n",
    "y_proba = model.predict(X_new)\n",
    "y_proba.round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 105
    },
    "colab_type": "code",
    "id": "lysdDpXXaXFY",
    "outputId": "c9cf3a43-402f-4df9-d64c-613735ce876a"
   },
   "outputs": [],
   "source": [
    "y_pred = model.predict_classes(X_new)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "_Ybn92SmonYj",
    "outputId": "e5f70328-795c-46df-e6fe-fdaf8768dd14"
   },
   "outputs": [],
   "source": [
    "y_pred = np.argmax(y_proba, axis=-1)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "zVDFbrnOaXN6",
    "outputId": "89b887de-d93e-41e8-c7fa-db20b49aecaf"
   },
   "outputs": [],
   "source": [
    "np.array(class_names)[y_pred]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 165
    },
    "colab_type": "code",
    "id": "ffbf9KFtadJY",
    "outputId": "fc07ec59-f1be-4098-cfdf-1228bf449177"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(7.2, 2.4))\n",
    "for index, image in enumerate(X_new):\n",
    "    plt.subplot(1, 3, index + 1)\n",
    "    plt.imshow(image, cmap=\"binary\", interpolation=\"nearest\")\n",
    "    plt.axis('off')\n",
    "    plt.title(class_names[y_test[index]], fontsize=12)\n",
    "plt.subplots_adjust(wspace=0.2, hspace=0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DpzyBn70ZigS"
   },
   "source": [
    "# Guardar y Cargar Modelos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "S__NXAnsxI_5"
   },
   "source": [
    "Guardar modelos es tan sencillo como hacer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nDg65TpBZkkH"
   },
   "outputs": [],
   "source": [
    "model.save(\"my_keras_model.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PyTFAQroxL2B"
   },
   "source": [
    "Que luego se puede cargar (por ejemplo en otra computadora, con otro script, para hacer predicciones) usando:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oT-6iPoPauFY"
   },
   "outputs": [],
   "source": [
    "model = keras.models.load_model(\"my_keras_model.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KrejS892xUQ5"
   },
   "source": [
    "También podemos guardar solamente los weights de los layers. Pero en este caso deberemos, antes de cargarlos, definir el mismo modelo que usamos y recien entonces cargar los weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dd0dte83axdC"
   },
   "outputs": [],
   "source": [
    "model.save_weights(\"my_keras_weights.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KCEBS3enaz0H"
   },
   "outputs": [],
   "source": [
    "model.load_weights(\"my_keras_weights.ckpt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FhizHlVnkjKR"
   },
   "source": [
    "# Regresión: California Housing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BdlNX14NxgdO"
   },
   "source": [
    "Veamos como se desempeña para hacer regresión con el dataset de california housing que utilizamos en la guía de ejercicios Datasets. Importamos los datos, y los escaleamos usando StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "8_g1GZM9km8Q",
    "outputId": "760b188a-a326-496f-9687-ee6a99ea0bf3"
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "housing = fetch_california_housing()\n",
    "\n",
    "X_train_full, X_test, y_train_full, y_test = train_test_split(housing.data, housing.target, random_state=42)\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X_train_full, y_train_full, random_state=42)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_valid = scaler.transform(X_valid)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EaLKkccNxptl"
   },
   "source": [
    "El modelo que definimos tiene como ultimo layer una funcion lineal, sin ninguna funcion de activación. (Es como hacer regresión lineal, pero fiteando unidades ReLu como funciones de base)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 714
    },
    "colab_type": "code",
    "id": "WFDtfmzxkyls",
    "outputId": "c4b1bb62-c3f3-47df-8d8e-8b38c7d286de"
   },
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Dense(30, activation=\"relu\", input_shape=X_train.shape[1:]),\n",
    "    keras.layers.Dense(1)\n",
    "])\n",
    "model.compile(loss=\"mean_squared_error\", optimizer=keras.optimizers.SGD(lr=1e-3))\n",
    "history = model.fit(X_train, y_train, epochs=20, validation_data=(X_valid, y_valid))\n",
    "mse_test = model.evaluate(X_test, y_test)\n",
    "X_new = X_test[:3]\n",
    "y_pred = model.predict(X_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 320
    },
    "colab_type": "code",
    "id": "ogy8zfnUk9lJ",
    "outputId": "1c8a5e88-ea5d-46ee-f9d4-da9bec5e40cf"
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(history.history).plot()\n",
    "plt.grid(True)\n",
    "plt.gca().set_ylim(0, 1)\n",
    "plt.show()\n",
    "print(y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "x5fQBF1XlAYM"
   },
   "source": [
    "## Ventajas de la API funcional: Modelos no secuenciales"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GKCoZmChx5rf"
   },
   "source": [
    "Aqui vemos un ejemplo de como podemos usar la API funcional para hacer modelos mas complejos. En el modelo [Wide and Deep](https://https://ai.google/research/pubs/pub45413) se conectan los inputs directamente a los outputs, ademas de pasando por una red profunda de dos capas. De esta manera, el modelo puede aprender features mas abstractos de los datos, y a la vez tomarlos directamente sin distorisión."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 340
    },
    "colab_type": "code",
    "id": "emM5moUylGFE",
    "outputId": "526593d1-31b6-454f-ed51-bd13f1a6ed74"
   },
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "input_ = keras.layers.Input(shape=X_train.shape[1:])\n",
    "hidden1 = keras.layers.Dense(30, activation=\"relu\")(input_)\n",
    "hidden2 = keras.layers.Dense(30, activation=\"relu\")(hidden1)\n",
    "concat = keras.layers.concatenate([input_, hidden2])\n",
    "output = keras.layers.Dense(1)(concat)\n",
    "model = keras.models.Model(inputs=[input_], outputs=[output])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 533
    },
    "colab_type": "code",
    "id": "lJBY5y7fpWc2",
    "outputId": "3243b481-eaf3-4cd8-f8f2-56e1f6fecac6"
   },
   "outputs": [],
   "source": [
    "keras.utils.plot_model(model, \"wide_and_deep_model.png\", show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 714
    },
    "colab_type": "code",
    "id": "AmapksBIlNWA",
    "outputId": "c8eec38d-e067-4fa7-9219-1ce1b8121268"
   },
   "outputs": [],
   "source": [
    "model.compile(loss=\"mean_squared_error\", optimizer=keras.optimizers.SGD(lr=1e-3))\n",
    "history = model.fit(X_train, y_train, epochs=20,\n",
    "                    validation_data=(X_valid, y_valid))\n",
    "mse_test = model.evaluate(X_test, y_test)\n",
    "y_pred = model.predict(X_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7hjgiSLzbRo1"
   },
   "source": [
    "# EXTRAS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "B0UQUJJOyVu0"
   },
   "source": [
    "Callbacks son una manera de hacer cosas durante el entrenamiento. Son utiles en caso que el modelo tarde mucho en fitear. Por ejemplo, podemos guardar el modelo al fin de cada epoca (por si se nos apaga la maquina en el medio del training), o implementar un regularizador EarlyStopping y dejar de fitear si el validation score no mejora, o enviarnos un mail cuando termine el entrenamiento con los resultados, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dW0PRpE-a10X"
   },
   "source": [
    "## Callbacks: Guardar durante entrenamiento (checkpoints)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JtJ5vPqplluY"
   },
   "outputs": [],
   "source": [
    "keras.backend.clear_session()\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Dense(30, activation=\"relu\", input_shape=[8]),\n",
    "    keras.layers.Dense(30, activation=\"relu\"),\n",
    "    keras.layers.Dense(1)\n",
    "])\n",
    "\n",
    "model.compile(loss=\"mse\", optimizer=keras.optimizers.SGD(lr=1e-3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 667
    },
    "colab_type": "code",
    "id": "6nY_ql6qa5ph",
    "outputId": "5bb59ed7-8637-4f22-f565-2905a53c9452"
   },
   "outputs": [],
   "source": [
    " checkpoint_cb = keras.callbacks.ModelCheckpoint(\"my_keras_model.h5\", save_best_only=True)\n",
    "\n",
    " history = model.fit(X_train, y_train, epochs=30,\n",
    "                    validation_data=(X_valid, y_valid),\n",
    "                     callbacks=[checkpoint_cb])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "o3OtrP40p__9"
   },
   "outputs": [],
   "source": [
    "model = keras.models.load_model(\"my_keras_model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "CpZxyWWqqCPQ",
    "outputId": "4af074dc-06ce-40ed-8c6e-f94355f17843"
   },
   "outputs": [],
   "source": [
    "model.evaluate(X_valid,y_valid) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "03oEvTcQbVcL"
   },
   "source": [
    "## Callbacks: Early Stopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iWppr7b4lvms"
   },
   "outputs": [],
   "source": [
    "keras.backend.clear_session()\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Dense(30, activation=\"relu\", input_shape=[8]),\n",
    "    keras.layers.Dense(30, activation=\"relu\"),\n",
    "    keras.layers.Dense(1)\n",
    "])\n",
    "\n",
    "model.compile(loss=\"mse\", optimizer=keras.optimizers.SGD(lr=1e-3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "A3bXGXaRbXIt"
   },
   "outputs": [],
   "source": [
    "model.compile(loss=\"mse\", optimizer=keras.optimizers.SGD(lr=1e-3))\n",
    "early_stopping_cb = keras.callbacks.EarlyStopping(patience=10,\n",
    "                                                  restore_best_weights=True)\n",
    "history = model.fit(X_train, y_train, epochs=100,\n",
    "                    validation_data=(X_valid, y_valid),\n",
    "                    callbacks=[checkpoint_cb, early_stopping_cb])\n",
    "mse_test = model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BmuTxeNCl6nc"
   },
   "source": [
    "## Busqueda de Hiperparámetros: Sklearn integration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "s5nqSWlNytYA"
   },
   "source": [
    "Y lo mejor de todo, tiene un wrapper que nos permite integrarlo con ScikitLearn. Sklearn lo reconocerá el modelo como uno más de los suyos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "db6zYIGMl5_B"
   },
   "outputs": [],
   "source": [
    "keras.backend.clear_session()\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "def build_model(n_hidden=1, n_neurons=30, learning_rate=3e-3, input_shape=[8]):\n",
    "    model = keras.models.Sequential()\n",
    "    model.add(keras.layers.InputLayer(input_shape=input_shape))\n",
    "    for layer in range(n_hidden):\n",
    "        model.add(keras.layers.Dense(n_neurons, activation=\"relu\"))\n",
    "    model.add(keras.layers.Dense(1))\n",
    "    optimizer = keras.optimizers.SGD(lr=learning_rate)\n",
    "    model.compile(loss=\"mse\", optimizer=optimizer)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wxMV0o_vmBDG"
   },
   "outputs": [],
   "source": [
    "keras_reg = keras.wrappers.scikit_learn.KerasRegressor(build_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 544
    },
    "colab_type": "code",
    "id": "7vaBNv0-mWV3",
    "outputId": "0a3ed3e5-559b-4473-c068-392f8a6c3076"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 1.1726 - val_loss: 2.5859\n",
      "Epoch 2/30\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.6369 - val_loss: 1.3919\n",
      "Epoch 3/30\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.5286 - val_loss: 1.1011\n",
      "Epoch 4/30\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.4911 - val_loss: 1.0702\n",
      "Epoch 5/30\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.4534 - val_loss: 0.5872\n",
      "Epoch 6/30\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.4365 - val_loss: 0.7241\n",
      "Epoch 7/30\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.4228 - val_loss: 0.5997\n",
      "Epoch 8/30\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.4148 - val_loss: 0.9960\n",
      "Epoch 9/30\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.4091 - val_loss: 0.8042\n",
      "Epoch 10/30\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.4049 - val_loss: 1.5251\n",
      "Epoch 11/30\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.4047 - val_loss: 2.5856\n",
      "Epoch 12/30\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.4195 - val_loss: 4.7300\n",
      "Epoch 13/30\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.4333 - val_loss: 2.5087\n",
      "Epoch 14/30\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.4057 - val_loss: 1.6190\n",
      "Epoch 15/30\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3971 - val_loss: 0.7897\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7ff74062d668>"
      ]
     },
     "execution_count": 58,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keras_reg.fit(X_train, y_train, epochs=30,\n",
    "              validation_data=(X_valid, y_valid),\n",
    "              callbacks=[keras.callbacks.EarlyStopping(patience=10)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "y_sHKiYYy7O7"
   },
   "source": [
    "El modelo tiene los métodos usuales de un estimador de sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "BrpGmS1FmcSC",
    "outputId": "b3546eeb-33ef-4247-e578-1640662c34da"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "162/162 [==============================] - 0s 1ms/step - loss: 0.3899\n"
     ]
    }
   ],
   "source": [
    "mse_test = keras_reg.score(X_test, y_test)\n",
    "y_pred = keras_reg.predict(X_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HUwX9oDNy2HY"
   },
   "source": [
    "Por ejemplo, podemos usarlo para buscar hiperparámetros con RandomizedSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "Mkr2t3Ivmnfz",
    "outputId": "541edafa-1a1d-4393-f44e-68b21d369d28"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 10 candidates, totalling 30 fits\n",
      "[CV] learning_rate=0.001683454924600351, n_hidden=0, n_neurons=15 ....\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "242/242 [==============================] - 1s 2ms/step - loss: 3.5557 - val_loss: 1.8752\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 1.3347 - val_loss: 0.9522\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.8591 - val_loss: 0.7820\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.7360 - val_loss: 0.7249\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.6930 - val_loss: 0.6994\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.6668 - val_loss: 0.9118\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.6514 - val_loss: 0.8495\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.6381 - val_loss: 0.8605\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.6276 - val_loss: 0.6524\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.6125 - val_loss: 0.8619\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.6057 - val_loss: 0.8659\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.5993 - val_loss: 0.5962\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.5859 - val_loss: 0.9062\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.5828 - val_loss: 0.9541\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.5799 - val_loss: 0.6402\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.5706 - val_loss: 0.7806\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.5670 - val_loss: 0.7985\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.5620 - val_loss: 0.8756\n",
      "Epoch 19/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.5585 - val_loss: 0.8958\n",
      "Epoch 20/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.5564 - val_loss: 0.8657\n",
      "Epoch 21/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.5559 - val_loss: 0.5940\n",
      "Epoch 22/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.5476 - val_loss: 0.8007\n",
      "Epoch 23/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.5484 - val_loss: 0.7792\n",
      "Epoch 24/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.5459 - val_loss: 0.7622\n",
      "Epoch 25/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.5453 - val_loss: 0.6476\n",
      "Epoch 26/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.5431 - val_loss: 0.5424\n",
      "Epoch 27/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.5373 - val_loss: 0.8687\n",
      "Epoch 28/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.5424 - val_loss: 0.5390\n",
      "Epoch 29/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.5365 - val_loss: 0.7179\n",
      "Epoch 30/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.5384 - val_loss: 0.6029\n",
      "Epoch 31/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.5362 - val_loss: 0.5947\n",
      "Epoch 32/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.5359 - val_loss: 0.5305\n",
      "Epoch 33/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.5334 - val_loss: 0.6601\n",
      "Epoch 34/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.5341 - val_loss: 0.6326\n",
      "Epoch 35/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.5344 - val_loss: 0.5072\n",
      "Epoch 36/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.5304 - val_loss: 0.7270\n",
      "Epoch 37/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.5341 - val_loss: 0.5055\n",
      "Epoch 38/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.5284 - val_loss: 0.7985\n",
      "Epoch 39/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.5338 - val_loss: 0.5176\n",
      "Epoch 40/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.5305 - val_loss: 0.5823\n",
      "Epoch 41/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.5293 - val_loss: 0.7114\n",
      "Epoch 42/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.5322 - val_loss: 0.5059\n",
      "Epoch 43/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.5302 - val_loss: 0.5008\n",
      "Epoch 44/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.5274 - val_loss: 0.7397\n",
      "Epoch 45/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.5309 - val_loss: 0.6169\n",
      "Epoch 46/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.5303 - val_loss: 0.5264\n",
      "Epoch 47/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.5276 - val_loss: 0.6916\n",
      "Epoch 48/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.5298 - val_loss: 0.6554\n",
      "Epoch 49/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.5290 - val_loss: 0.6607\n",
      "Epoch 50/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.5267 - val_loss: 0.8497\n",
      "Epoch 51/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.5310 - val_loss: 0.6664\n",
      "Epoch 52/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.5294 - val_loss: 0.5996\n",
      "Epoch 53/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.5282 - val_loss: 0.6414\n",
      "121/121 [==============================] - 0s 1ms/step - loss: 0.5368\n",
      "[CV]  learning_rate=0.001683454924600351, n_hidden=0, n_neurons=15, total=  30.5s\n",
      "[CV] learning_rate=0.001683454924600351, n_hidden=0, n_neurons=15 ....\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:   30.5s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "242/242 [==============================] - 1s 2ms/step - loss: 3.5605 - val_loss: 23.0855\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 1.4777 - val_loss: 10.8387\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 1.0149 - val_loss: 4.4392\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.8729 - val_loss: 1.5338\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.8027 - val_loss: 0.7192\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.7542 - val_loss: 1.2046\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.7160 - val_loss: 2.4524\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.6847 - val_loss: 4.1421\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.6588 - val_loss: 5.9820\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.6371 - val_loss: 7.7654\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.6187 - val_loss: 9.6230\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.6029 - val_loss: 11.3609\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.5896 - val_loss: 12.9821\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.5781 - val_loss: 14.2266\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.5683 - val_loss: 15.4321\n",
      "121/121 [==============================] - 0s 1ms/step - loss: 0.9198\n",
      "[CV]  learning_rate=0.001683454924600351, n_hidden=0, n_neurons=15, total=   8.8s\n",
      "[CV] learning_rate=0.001683454924600351, n_hidden=0, n_neurons=15 ....\n",
      "Epoch 1/100\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 3.2972 - val_loss: 1.3307\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.9648 - val_loss: 0.6934\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.6150 - val_loss: 0.5469\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.5468 - val_loss: 0.7322\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.5372 - val_loss: 0.4963\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.5330 - val_loss: 0.5539\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.5320 - val_loss: 0.5729\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.5297 - val_loss: 0.7873\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.5337 - val_loss: 0.5968\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.5314 - val_loss: 0.4951\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.5286 - val_loss: 0.7591\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.5333 - val_loss: 0.5368\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.5305 - val_loss: 0.4968\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.5305 - val_loss: 0.5778\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.5313 - val_loss: 0.5117\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.5282 - val_loss: 0.7055\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.5320 - val_loss: 0.5399\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.5307 - val_loss: 0.5257\n",
      "Epoch 19/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.5275 - val_loss: 0.7902\n",
      "Epoch 20/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.5327 - val_loss: 0.5852\n",
      "121/121 [==============================] - 0s 1ms/step - loss: 0.5317\n",
      "[CV]  learning_rate=0.001683454924600351, n_hidden=0, n_neurons=15, total=  11.7s\n",
      "[CV] learning_rate=0.008731907739399206, n_hidden=0, n_neurons=21 ....\n",
      "Epoch 1/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 1.4256 - val_loss: 66.5657\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.9941 - val_loss: 137.1490\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 2.2587 - val_loss: 716.1611\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 4.3545 - val_loss: 2297.8604\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 17.0750 - val_loss: 9988.3398\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 198.7058 - val_loss: 39231.9727\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 424.9947 - val_loss: 155196.9219\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 2992.7771 - val_loss: 612492.9375\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 7662.3345 - val_loss: 2435756.2500\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 53693.9375 - val_loss: 10128962.0000\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 135637.9062 - val_loss: 39694520.0000\n",
      "121/121 [==============================] - 0s 1ms/step - loss: 105477.4922\n",
      "[CV]  learning_rate=0.008731907739399206, n_hidden=0, n_neurons=21, total=   6.6s\n",
      "[CV] learning_rate=0.008731907739399206, n_hidden=0, n_neurons=21 ....\n",
      "Epoch 1/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 1.1573 - val_loss: 23.1193\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.5349 - val_loss: 22.1675\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.5192 - val_loss: 22.3752\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.5148 - val_loss: 21.3891\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.5108 - val_loss: 20.8855\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.5082 - val_loss: 20.6379\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.5070 - val_loss: 20.0736\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.5050 - val_loss: 20.7178\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.5029 - val_loss: 20.0844\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.5032 - val_loss: 17.0622\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.5036 - val_loss: 19.1666\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.5017 - val_loss: 20.8246\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.5023 - val_loss: 22.0298\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.5049 - val_loss: 17.6022\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.5024 - val_loss: 18.6171\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.5024 - val_loss: 20.0451\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.5005 - val_loss: 17.5898\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.5040 - val_loss: 17.4526\n",
      "Epoch 19/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.5012 - val_loss: 19.5015\n",
      "Epoch 20/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.5011 - val_loss: 17.3223\n",
      "121/121 [==============================] - 0s 1ms/step - loss: 0.9327\n",
      "[CV]  learning_rate=0.008731907739399206, n_hidden=0, n_neurons=21, total=  11.6s\n",
      "[CV] learning_rate=0.008731907739399206, n_hidden=0, n_neurons=21 ....\n",
      "Epoch 1/100\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 1.4616 - val_loss: 0.5742\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.6113 - val_loss: 6.7367\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.5784 - val_loss: 6.5227\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.5820 - val_loss: 19.7082\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.6738 - val_loss: 205.7214\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 1.6846 - val_loss: 282.6046\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 2.5718 - val_loss: 656.3253\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 12.3829 - val_loss: 1380.0121\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 14.8443 - val_loss: 2817.4539\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 7.4320 - val_loss: 4499.3813\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 121.3308 - val_loss: 8457.8711\n",
      "121/121 [==============================] - 0s 1ms/step - loss: 11.0521\n",
      "[CV]  learning_rate=0.008731907739399206, n_hidden=0, n_neurons=21, total=   6.6s\n",
      "[CV] learning_rate=0.0006154014789262348, n_hidden=2, n_neurons=87 ...\n",
      "Epoch 1/100\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 2.5089 - val_loss: 2.6033\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 1.0793 - val_loss: 1.0424\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 0.8038 - val_loss: 0.7507\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.7203 - val_loss: 0.6758\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.6785 - val_loss: 0.6484\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.6498 - val_loss: 0.6241\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 0.6261 - val_loss: 0.6073\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 0.6055 - val_loss: 0.5826\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.5870 - val_loss: 0.5597\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.5700 - val_loss: 0.5445\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.5547 - val_loss: 0.5314\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.5408 - val_loss: 0.5147\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.5278 - val_loss: 0.5030\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.5159 - val_loss: 0.4904\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.5051 - val_loss: 0.4791\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.4948 - val_loss: 0.4695\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 0.4857 - val_loss: 0.4608\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.4772 - val_loss: 0.4524\n",
      "Epoch 19/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.4691 - val_loss: 0.4476\n",
      "Epoch 20/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.4621 - val_loss: 0.4383\n",
      "Epoch 21/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.4555 - val_loss: 0.4355\n",
      "Epoch 22/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.4494 - val_loss: 0.4282\n",
      "Epoch 23/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.4437 - val_loss: 0.4230\n",
      "Epoch 24/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.4384 - val_loss: 0.4166\n",
      "Epoch 25/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.4339 - val_loss: 0.4161\n",
      "Epoch 26/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.4294 - val_loss: 0.4142\n",
      "Epoch 27/100\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 0.4254 - val_loss: 0.4100\n",
      "Epoch 28/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.4217 - val_loss: 0.4132\n",
      "Epoch 29/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.4180 - val_loss: 0.4103\n",
      "Epoch 30/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.4149 - val_loss: 0.4032\n",
      "Epoch 31/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.4118 - val_loss: 0.3964\n",
      "Epoch 32/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.4089 - val_loss: 0.3956\n",
      "Epoch 33/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.4062 - val_loss: 0.4013\n",
      "Epoch 34/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.4035 - val_loss: 0.4004\n",
      "Epoch 35/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.4012 - val_loss: 0.3913\n",
      "Epoch 36/100\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 0.3988 - val_loss: 0.3986\n",
      "Epoch 37/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3968 - val_loss: 0.3871\n",
      "Epoch 38/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3946 - val_loss: 0.3998\n",
      "Epoch 39/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3927 - val_loss: 0.3858\n",
      "Epoch 40/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3908 - val_loss: 0.3967\n",
      "Epoch 41/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3890 - val_loss: 0.3918\n",
      "Epoch 42/100\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 0.3873 - val_loss: 0.3866\n",
      "Epoch 43/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3858 - val_loss: 0.3800\n",
      "Epoch 44/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3840 - val_loss: 0.3997\n",
      "Epoch 45/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3827 - val_loss: 0.3861\n",
      "Epoch 46/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3811 - val_loss: 0.3805\n",
      "Epoch 47/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3799 - val_loss: 0.3919\n",
      "Epoch 48/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3785 - val_loss: 0.3826\n",
      "Epoch 49/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3772 - val_loss: 0.3812\n",
      "Epoch 50/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3759 - val_loss: 0.3905\n",
      "Epoch 51/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3746 - val_loss: 0.3832\n",
      "Epoch 52/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3737 - val_loss: 0.3827\n",
      "Epoch 53/100\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 0.3725 - val_loss: 0.3859\n",
      "121/121 [==============================] - 0s 1ms/step - loss: 0.3865\n",
      "[CV]  learning_rate=0.0006154014789262348, n_hidden=2, n_neurons=87, total=  32.3s\n",
      "[CV] learning_rate=0.0006154014789262348, n_hidden=2, n_neurons=87 ...\n",
      "Epoch 1/100\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 2.7762 - val_loss: 17.5435\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 1.1017 - val_loss: 15.4502\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.8039 - val_loss: 11.1084\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.7051 - val_loss: 8.0885\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.6575 - val_loss: 6.1076\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 0.6260 - val_loss: 4.7302\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.6007 - val_loss: 3.6783\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.5790 - val_loss: 2.8274\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.5600 - val_loss: 2.2526\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.5433 - val_loss: 1.7966\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 0.5284 - val_loss: 1.4646\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.5153 - val_loss: 1.1656\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.5036 - val_loss: 0.9599\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 0.4932 - val_loss: 0.8400\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.4839 - val_loss: 0.7148\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 0.4756 - val_loss: 0.6408\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.4680 - val_loss: 0.5679\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.4612 - val_loss: 0.5264\n",
      "Epoch 19/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.4549 - val_loss: 0.4894\n",
      "Epoch 20/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.4493 - val_loss: 0.4711\n",
      "Epoch 21/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.4442 - val_loss: 0.4525\n",
      "Epoch 22/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.4395 - val_loss: 0.4467\n",
      "Epoch 23/100\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 0.4352 - val_loss: 0.4404\n",
      "Epoch 24/100\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 0.4313 - val_loss: 0.4333\n",
      "Epoch 25/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.4276 - val_loss: 0.4303\n",
      "Epoch 26/100\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 0.4242 - val_loss: 0.4284\n",
      "Epoch 27/100\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 0.4209 - val_loss: 0.4270\n",
      "Epoch 28/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.4180 - val_loss: 0.4269\n",
      "Epoch 29/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.4148 - val_loss: 0.4416\n",
      "Epoch 30/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.4127 - val_loss: 0.4363\n",
      "Epoch 31/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.4099 - val_loss: 0.4330\n",
      "Epoch 32/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.4074 - val_loss: 0.4407\n",
      "Epoch 33/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.4052 - val_loss: 0.4484\n",
      "Epoch 34/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.4029 - val_loss: 0.4646\n",
      "Epoch 35/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.4009 - val_loss: 0.4789\n",
      "Epoch 36/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3989 - val_loss: 0.4745\n",
      "Epoch 37/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3970 - val_loss: 0.4974\n",
      "Epoch 38/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3951 - val_loss: 0.5135\n",
      "121/121 [==============================] - 0s 1ms/step - loss: 0.4088\n",
      "[CV]  learning_rate=0.0006154014789262348, n_hidden=2, n_neurons=87, total=  23.3s\n",
      "[CV] learning_rate=0.0006154014789262348, n_hidden=2, n_neurons=87 ...\n",
      "Epoch 1/100\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 2.8501 - val_loss: 2.0961\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 1.1187 - val_loss: 1.2079\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 0.8431 - val_loss: 0.8075\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.7630 - val_loss: 0.7207\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.7220 - val_loss: 0.6952\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.6925 - val_loss: 0.6614\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.6677 - val_loss: 0.6378\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.6461 - val_loss: 0.6132\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.6268 - val_loss: 0.6043\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.6081 - val_loss: 0.5937\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.5908 - val_loss: 0.5658\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.5749 - val_loss: 0.5551\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.5601 - val_loss: 0.5476\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.5465 - val_loss: 0.5450\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.5340 - val_loss: 0.5314\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 0.5225 - val_loss: 0.5067\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 0.5119 - val_loss: 0.4983\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 0.5020 - val_loss: 0.4873\n",
      "Epoch 19/100\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 0.4930 - val_loss: 0.4748\n",
      "Epoch 20/100\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 0.4849 - val_loss: 0.4767\n",
      "Epoch 21/100\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 0.4771 - val_loss: 0.4719\n",
      "Epoch 22/100\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 0.4701 - val_loss: 0.4623\n",
      "Epoch 23/100\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 0.4637 - val_loss: 0.4640\n",
      "Epoch 24/100\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 0.4575 - val_loss: 0.4777\n",
      "Epoch 25/100\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 0.4521 - val_loss: 0.4488\n",
      "Epoch 26/100\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 0.4469 - val_loss: 0.4475\n",
      "Epoch 27/100\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 0.4423 - val_loss: 0.4420\n",
      "Epoch 28/100\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 0.4378 - val_loss: 0.4449\n",
      "Epoch 29/100\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 0.4337 - val_loss: 0.4581\n",
      "Epoch 30/100\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 0.4301 - val_loss: 0.4385\n",
      "Epoch 31/100\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 0.4265 - val_loss: 0.4226\n",
      "Epoch 32/100\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 0.4233 - val_loss: 0.4458\n",
      "Epoch 33/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.4201 - val_loss: 0.4242\n",
      "Epoch 34/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.4174 - val_loss: 0.4542\n",
      "Epoch 35/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.4149 - val_loss: 0.4279\n",
      "Epoch 36/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.4122 - val_loss: 0.4341\n",
      "Epoch 37/100\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 0.4101 - val_loss: 0.4189\n",
      "Epoch 38/100\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 0.4078 - val_loss: 0.4344\n",
      "Epoch 39/100\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 0.4056 - val_loss: 0.4235\n",
      "Epoch 40/100\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 0.4037 - val_loss: 0.4183\n",
      "Epoch 41/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.4015 - val_loss: 0.4552\n",
      "Epoch 42/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.4001 - val_loss: 0.4411\n",
      "Epoch 43/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3981 - val_loss: 0.4073\n",
      "Epoch 44/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3967 - val_loss: 0.4294\n",
      "Epoch 45/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3950 - val_loss: 0.4238\n",
      "Epoch 46/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3936 - val_loss: 0.4128\n",
      "Epoch 47/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3921 - val_loss: 0.3977\n",
      "Epoch 48/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3909 - val_loss: 0.4028\n",
      "Epoch 49/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3893 - val_loss: 0.4362\n",
      "Epoch 50/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3884 - val_loss: 0.4235\n",
      "Epoch 51/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3870 - val_loss: 0.4171\n",
      "Epoch 52/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3859 - val_loss: 0.4273\n",
      "Epoch 53/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3849 - val_loss: 0.4076\n",
      "Epoch 54/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3839 - val_loss: 0.3885\n",
      "Epoch 55/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3827 - val_loss: 0.4003\n",
      "Epoch 56/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3816 - val_loss: 0.4176\n",
      "Epoch 57/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3808 - val_loss: 0.4201\n",
      "Epoch 58/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3799 - val_loss: 0.4177\n",
      "Epoch 59/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3789 - val_loss: 0.4166\n",
      "Epoch 60/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3779 - val_loss: 0.3910\n",
      "Epoch 61/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3772 - val_loss: 0.4094\n",
      "Epoch 62/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3763 - val_loss: 0.4364\n",
      "Epoch 63/100\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 0.3757 - val_loss: 0.4025\n",
      "Epoch 64/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3749 - val_loss: 0.4028\n",
      "121/121 [==============================] - 0s 1ms/step - loss: 0.3737\n",
      "[CV]  learning_rate=0.0006154014789262348, n_hidden=2, n_neurons=87, total=  39.4s\n",
      "[CV] learning_rate=0.0003920021771415983, n_hidden=3, n_neurons=24 ...\n",
      "Epoch 1/100\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 2.4720 - val_loss: 7.9723\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 1.1323 - val_loss: 5.6563\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.8832 - val_loss: 4.1443\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.8066 - val_loss: 3.1169\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.7657 - val_loss: 2.6199\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 0.7374 - val_loss: 2.2830\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.7171 - val_loss: 1.9726\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 0.6983 - val_loss: 1.7536\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.6816 - val_loss: 1.5653\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.6663 - val_loss: 1.4316\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 0.6521 - val_loss: 1.3165\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.6387 - val_loss: 1.2101\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 0.6261 - val_loss: 1.1236\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.6139 - val_loss: 1.0591\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.6024 - val_loss: 0.9875\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 0.5910 - val_loss: 0.9345\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.5804 - val_loss: 0.8832\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 0.5701 - val_loss: 0.8424\n",
      "Epoch 19/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.5602 - val_loss: 0.8079\n",
      "Epoch 20/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.5511 - val_loss: 0.7646\n",
      "Epoch 21/100\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 0.5420 - val_loss: 0.7347\n",
      "Epoch 22/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.5332 - val_loss: 0.7075\n",
      "Epoch 23/100\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 0.5249 - val_loss: 0.6815\n",
      "Epoch 24/100\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 0.5165 - val_loss: 0.6537\n",
      "Epoch 25/100\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 0.5095 - val_loss: 0.6361\n",
      "Epoch 26/100\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 0.5020 - val_loss: 0.6174\n",
      "Epoch 27/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.4952 - val_loss: 0.6011\n",
      "Epoch 28/100\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 0.4887 - val_loss: 0.5887\n",
      "Epoch 29/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.4824 - val_loss: 0.5778\n",
      "Epoch 30/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.4766 - val_loss: 0.5671\n",
      "Epoch 31/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.4710 - val_loss: 0.5557\n",
      "Epoch 32/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.4658 - val_loss: 0.5475\n",
      "Epoch 33/100\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 0.4608 - val_loss: 0.5403\n",
      "Epoch 34/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.4561 - val_loss: 0.5322\n",
      "Epoch 35/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.4516 - val_loss: 0.5250\n",
      "Epoch 36/100\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 0.4476 - val_loss: 0.5165\n",
      "Epoch 37/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.4436 - val_loss: 0.5106\n",
      "Epoch 38/100\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 0.4400 - val_loss: 0.5053\n",
      "Epoch 39/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.4364 - val_loss: 0.5004\n",
      "Epoch 40/100\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 0.4333 - val_loss: 0.4966\n",
      "Epoch 41/100\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 0.4302 - val_loss: 0.4922\n",
      "Epoch 42/100\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 0.4274 - val_loss: 0.4891\n",
      "Epoch 43/100\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 0.4248 - val_loss: 0.4850\n",
      "Epoch 44/100\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 0.4220 - val_loss: 0.4854\n",
      "Epoch 45/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.4198 - val_loss: 0.4828\n",
      "Epoch 46/100\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 0.4173 - val_loss: 0.4779\n",
      "Epoch 47/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.4153 - val_loss: 0.4783\n",
      "Epoch 48/100\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 0.4131 - val_loss: 0.4755\n",
      "Epoch 49/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.4111 - val_loss: 0.4765\n",
      "Epoch 50/100\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 0.4093 - val_loss: 0.4753\n",
      "Epoch 51/100\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 0.4074 - val_loss: 0.4714\n",
      "Epoch 52/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.4060 - val_loss: 0.4726\n",
      "Epoch 53/100\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 0.4043 - val_loss: 0.4721\n",
      "Epoch 54/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.4027 - val_loss: 0.4708\n",
      "Epoch 55/100\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 0.4009 - val_loss: 0.4703\n",
      "Epoch 56/100\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 0.3998 - val_loss: 0.4713\n",
      "Epoch 57/100\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 0.3985 - val_loss: 0.4704\n",
      "Epoch 58/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3970 - val_loss: 0.4718\n",
      "Epoch 59/100\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 0.3958 - val_loss: 0.4712\n",
      "Epoch 60/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3944 - val_loss: 0.4701\n",
      "Epoch 61/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3936 - val_loss: 0.4718\n",
      "Epoch 62/100\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 0.3924 - val_loss: 0.4716\n",
      "Epoch 63/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3913 - val_loss: 0.4704\n",
      "Epoch 64/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3903 - val_loss: 0.4735\n",
      "Epoch 65/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3892 - val_loss: 0.4738\n",
      "Epoch 66/100\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 0.3884 - val_loss: 0.4729\n",
      "Epoch 67/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3873 - val_loss: 0.4716\n",
      "Epoch 68/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3864 - val_loss: 0.4731\n",
      "Epoch 69/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3855 - val_loss: 0.4720\n",
      "Epoch 70/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3846 - val_loss: 0.4721\n",
      "121/121 [==============================] - 0s 1ms/step - loss: 0.4001\n",
      "[CV]  learning_rate=0.0003920021771415983, n_hidden=3, n_neurons=24, total=  43.2s\n",
      "[CV] learning_rate=0.0003920021771415983, n_hidden=3, n_neurons=24 ...\n",
      "Epoch 1/100\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 3.7641 - val_loss: 28.0492\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 2.0504 - val_loss: 43.0472\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 1.6124 - val_loss: 37.0128\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 1.3603 - val_loss: 28.7538\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 1.1689 - val_loss: 20.6120\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 1.0259 - val_loss: 14.6245\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.9261 - val_loss: 10.5960\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.8594 - val_loss: 7.2861\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.8137 - val_loss: 5.1836\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.7810 - val_loss: 3.7344\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 0.7555 - val_loss: 2.7778\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 0.7346 - val_loss: 1.9391\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.7165 - val_loss: 1.4673\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.7002 - val_loss: 1.2321\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 0.6852 - val_loss: 0.9812\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.6712 - val_loss: 0.8534\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.6583 - val_loss: 0.7166\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.6459 - val_loss: 0.6424\n",
      "Epoch 19/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.6340 - val_loss: 0.5949\n",
      "Epoch 20/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.6226 - val_loss: 0.5764\n",
      "Epoch 21/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.6116 - val_loss: 0.5809\n",
      "Epoch 22/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.6010 - val_loss: 0.6027\n",
      "Epoch 23/100\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 0.5911 - val_loss: 0.6369\n",
      "Epoch 24/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.5816 - val_loss: 0.6922\n",
      "Epoch 25/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.5725 - val_loss: 0.7604\n",
      "Epoch 26/100\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 0.5639 - val_loss: 0.8304\n",
      "Epoch 27/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.5555 - val_loss: 0.8810\n",
      "Epoch 28/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.5477 - val_loss: 0.9624\n",
      "Epoch 29/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.5401 - val_loss: 0.9578\n",
      "Epoch 30/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.5332 - val_loss: 1.0158\n",
      "121/121 [==============================] - 0s 1ms/step - loss: 0.5490\n",
      "[CV]  learning_rate=0.0003920021771415983, n_hidden=3, n_neurons=24, total=  18.6s\n",
      "[CV] learning_rate=0.0003920021771415983, n_hidden=3, n_neurons=24 ...\n",
      "Epoch 1/100\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 2.9218 - val_loss: 4.3285\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 1.2869 - val_loss: 2.8653\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 0.9733 - val_loss: 1.8260\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.8526 - val_loss: 1.2974\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.7870 - val_loss: 0.9606\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.7448 - val_loss: 0.7924\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.7141 - val_loss: 0.7158\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.6902 - val_loss: 0.6616\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 0.6699 - val_loss: 0.6363\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.6525 - val_loss: 0.6160\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 0.6364 - val_loss: 0.5999\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 0.6216 - val_loss: 0.5855\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.6084 - val_loss: 0.5729\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.5959 - val_loss: 0.5615\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.5844 - val_loss: 0.5509\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.5739 - val_loss: 0.5399\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.5641 - val_loss: 0.5301\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.5547 - val_loss: 0.5210\n",
      "Epoch 19/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.5461 - val_loss: 0.5129\n",
      "Epoch 20/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.5381 - val_loss: 0.5062\n",
      "Epoch 21/100\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 0.5305 - val_loss: 0.4992\n",
      "Epoch 22/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.5234 - val_loss: 0.4932\n",
      "Epoch 23/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.5169 - val_loss: 0.4875\n",
      "Epoch 24/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.5106 - val_loss: 0.4857\n",
      "Epoch 25/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.5050 - val_loss: 0.4783\n",
      "Epoch 26/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.4996 - val_loss: 0.4746\n",
      "Epoch 27/100\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 0.4945 - val_loss: 0.4700\n",
      "Epoch 28/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.4897 - val_loss: 0.4676\n",
      "Epoch 29/100\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 0.4851 - val_loss: 0.4687\n",
      "Epoch 30/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.4810 - val_loss: 0.4618\n",
      "Epoch 31/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.4771 - val_loss: 0.4607\n",
      "Epoch 32/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.4732 - val_loss: 0.4630\n",
      "Epoch 33/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.4697 - val_loss: 0.4583\n",
      "Epoch 34/100\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 0.4663 - val_loss: 0.4643\n",
      "Epoch 35/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.4633 - val_loss: 0.4591\n",
      "Epoch 36/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.4603 - val_loss: 0.4562\n",
      "Epoch 37/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.4576 - val_loss: 0.4539\n",
      "Epoch 38/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.4550 - val_loss: 0.4547\n",
      "Epoch 39/100\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 0.4525 - val_loss: 0.4534\n",
      "Epoch 40/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.4502 - val_loss: 0.4523\n",
      "Epoch 41/100\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 0.4478 - val_loss: 0.4613\n",
      "Epoch 42/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.4460 - val_loss: 0.4593\n",
      "Epoch 43/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.4438 - val_loss: 0.4497\n",
      "Epoch 44/100\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 0.4420 - val_loss: 0.4544\n",
      "Epoch 45/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.4402 - val_loss: 0.4533\n",
      "Epoch 46/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.4383 - val_loss: 0.4497\n",
      "Epoch 47/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.4367 - val_loss: 0.4470\n",
      "Epoch 48/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.4352 - val_loss: 0.4470\n",
      "Epoch 49/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.4336 - val_loss: 0.4532\n",
      "Epoch 50/100\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 0.4322 - val_loss: 0.4549\n",
      "Epoch 51/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.4307 - val_loss: 0.4534\n",
      "Epoch 52/100\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 0.4295 - val_loss: 0.4593\n",
      "Epoch 53/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.4281 - val_loss: 0.4535\n",
      "Epoch 54/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.4269 - val_loss: 0.4484\n",
      "Epoch 55/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.4255 - val_loss: 0.4489\n",
      "Epoch 56/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.4244 - val_loss: 0.4465\n",
      "Epoch 57/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.4232 - val_loss: 0.4489\n",
      "Epoch 58/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.4221 - val_loss: 0.4514\n",
      "Epoch 59/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.4209 - val_loss: 0.4499\n",
      "Epoch 60/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.4198 - val_loss: 0.4441\n",
      "Epoch 61/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.4188 - val_loss: 0.4476\n",
      "Epoch 62/100\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 0.4178 - val_loss: 0.4501\n",
      "Epoch 63/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.4170 - val_loss: 0.4431\n",
      "Epoch 64/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.4160 - val_loss: 0.4385\n",
      "Epoch 65/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.4151 - val_loss: 0.4381\n",
      "Epoch 66/100\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 0.4141 - val_loss: 0.4440\n",
      "Epoch 67/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.4134 - val_loss: 0.4354\n",
      "Epoch 68/100\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 0.4126 - val_loss: 0.4381\n",
      "Epoch 69/100\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 0.4117 - val_loss: 0.4341\n",
      "Epoch 70/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.4109 - val_loss: 0.4395\n",
      "Epoch 71/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.4101 - val_loss: 0.4340\n",
      "Epoch 72/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.4094 - val_loss: 0.4407\n",
      "Epoch 73/100\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 0.4084 - val_loss: 0.4309\n",
      "Epoch 74/100\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 0.4079 - val_loss: 0.4328\n",
      "Epoch 75/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.4071 - val_loss: 0.4349\n",
      "Epoch 76/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.4063 - val_loss: 0.4346\n",
      "Epoch 77/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.4056 - val_loss: 0.4339\n",
      "Epoch 78/100\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 0.4049 - val_loss: 0.4333\n",
      "Epoch 79/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.4042 - val_loss: 0.4281\n",
      "Epoch 80/100\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 0.4035 - val_loss: 0.4354\n",
      "Epoch 81/100\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 0.4029 - val_loss: 0.4322\n",
      "Epoch 82/100\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 0.4022 - val_loss: 0.4299\n",
      "Epoch 83/100\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 0.4015 - val_loss: 0.4292\n",
      "Epoch 84/100\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 0.4008 - val_loss: 0.4341\n",
      "Epoch 85/100\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 0.4000 - val_loss: 0.4219\n",
      "Epoch 86/100\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 0.3995 - val_loss: 0.4290\n",
      "Epoch 87/100\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 0.3987 - val_loss: 0.4329\n",
      "Epoch 88/100\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 0.3982 - val_loss: 0.4306\n",
      "Epoch 89/100\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 0.3975 - val_loss: 0.4337\n",
      "Epoch 90/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3968 - val_loss: 0.4250\n",
      "Epoch 91/100\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 0.3962 - val_loss: 0.4237\n",
      "Epoch 92/100\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 0.3957 - val_loss: 0.4191\n",
      "Epoch 93/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3950 - val_loss: 0.4238\n",
      "Epoch 94/100\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 0.3943 - val_loss: 0.4203\n",
      "Epoch 95/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3937 - val_loss: 0.4209\n",
      "Epoch 96/100\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 0.3932 - val_loss: 0.4215\n",
      "Epoch 97/100\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 0.3926 - val_loss: 0.4250\n",
      "Epoch 98/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3920 - val_loss: 0.4160\n",
      "Epoch 99/100\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 0.3915 - val_loss: 0.4199\n",
      "Epoch 100/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3910 - val_loss: 0.4242\n",
      "121/121 [==============================] - 0s 1ms/step - loss: 0.3897\n",
      "[CV]  learning_rate=0.0003920021771415983, n_hidden=3, n_neurons=24, total= 1.0min\n",
      "[CV] learning_rate=0.006010328378268217, n_hidden=0, n_neurons=2 .....\n",
      "Epoch 1/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 2.1013 - val_loss: 5.2312\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.8603 - val_loss: 26.5013\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.7494 - val_loss: 40.6122\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 1.0991 - val_loss: 135.6917\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 1.3388 - val_loss: 237.1149\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 4.6734 - val_loss: 506.5568\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 5.0735 - val_loss: 1165.5585\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 19.0953 - val_loss: 2646.9744\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 28.1002 - val_loss: 5780.9751\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 97.2630 - val_loss: 13751.4141\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 159.4888 - val_loss: 31633.9141\n",
      "121/121 [==============================] - 0s 1ms/step - loss: 81.5957\n",
      "[CV]  learning_rate=0.006010328378268217, n_hidden=0, n_neurons=2, total=   6.5s\n",
      "[CV] learning_rate=0.006010328378268217, n_hidden=0, n_neurons=2 .....\n",
      "Epoch 1/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 1.4769 - val_loss: 14.0701\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.5769 - val_loss: 16.8410\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.5493 - val_loss: 19.0635\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.5365 - val_loss: 19.7342\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.5272 - val_loss: 20.0593\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.5202 - val_loss: 20.2376\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.5153 - val_loss: 20.0296\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.5113 - val_loss: 20.3793\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.5078 - val_loss: 20.1103\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.5063 - val_loss: 18.4892\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.5056 - val_loss: 19.4013\n",
      "121/121 [==============================] - 0s 1ms/step - loss: 0.9640\n",
      "[CV]  learning_rate=0.006010328378268217, n_hidden=0, n_neurons=2, total=   6.3s\n",
      "[CV] learning_rate=0.006010328378268217, n_hidden=0, n_neurons=2 .....\n",
      "Epoch 1/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 2.0333 - val_loss: 13.7380\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.6240 - val_loss: 10.0594\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.7131 - val_loss: 41.2693\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 1.1121 - val_loss: 74.9048\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.9784 - val_loss: 205.5686\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 1.9726 - val_loss: 246.7374\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 2.5115 - val_loss: 388.8353\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 5.9673 - val_loss: 620.5347\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 6.9990 - val_loss: 919.7248\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 4.1843 - val_loss: 1082.5530\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 21.4948 - val_loss: 1471.0371\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 10.2613 - val_loss: 1957.3098\n",
      "121/121 [==============================] - 0s 1ms/step - loss: 2.0491\n",
      "[CV]  learning_rate=0.006010328378268217, n_hidden=0, n_neurons=2, total=   6.8s\n",
      "[CV] learning_rate=0.008339092654580042, n_hidden=1, n_neurons=38 ....\n",
      "Epoch 1/100\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 1.2457 - val_loss: 22.8634\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 1.1255 - val_loss: 36.5661\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.9314 - val_loss: 304.7440\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.8113 - val_loss: 71.4703\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.8365 - val_loss: 312.6019\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 2.8876 - val_loss: 0.4035\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.4331 - val_loss: 0.3815\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3841 - val_loss: 0.3614\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3771 - val_loss: 0.3613\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3676 - val_loss: 0.3455\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3700 - val_loss: 0.3469\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3623 - val_loss: 0.3554\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3548 - val_loss: 0.3456\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3603 - val_loss: 0.3430\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3515 - val_loss: 0.3427\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3531 - val_loss: 0.3421\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3472 - val_loss: 0.3402\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3515 - val_loss: 0.3426\n",
      "Epoch 19/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3628 - val_loss: 0.3416\n",
      "Epoch 20/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3443 - val_loss: 0.3385\n",
      "Epoch 21/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3431 - val_loss: 0.3428\n",
      "Epoch 22/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3416 - val_loss: 0.3400\n",
      "Epoch 23/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3398 - val_loss: 0.3584\n",
      "Epoch 24/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3413 - val_loss: 0.3509\n",
      "Epoch 25/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3375 - val_loss: 0.3590\n",
      "Epoch 26/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3367 - val_loss: 0.3456\n",
      "Epoch 27/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3369 - val_loss: 0.3546\n",
      "Epoch 28/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3367 - val_loss: 0.3542\n",
      "Epoch 29/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3350 - val_loss: 0.3522\n",
      "Epoch 30/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3347 - val_loss: 0.3408\n",
      "121/121 [==============================] - 0s 1ms/step - loss: 0.3580\n",
      "[CV]  learning_rate=0.008339092654580042, n_hidden=1, n_neurons=38, total=  17.2s\n",
      "[CV] learning_rate=0.008339092654580042, n_hidden=1, n_neurons=38 ....\n",
      "Epoch 1/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.8950 - val_loss: 3.0949\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.5223 - val_loss: 0.4712\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.4610 - val_loss: 0.4231\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.4356 - val_loss: 0.4021\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.4204 - val_loss: 0.4323\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.4116 - val_loss: 0.6513\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.4057 - val_loss: 0.8508\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.4000 - val_loss: 1.0201\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3954 - val_loss: 1.1757\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3924 - val_loss: 0.8698\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3877 - val_loss: 0.9377\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3839 - val_loss: 1.0793\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3820 - val_loss: 1.1923\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3805 - val_loss: 1.1186\n",
      "121/121 [==============================] - 0s 1ms/step - loss: 0.4037\n",
      "[CV]  learning_rate=0.008339092654580042, n_hidden=1, n_neurons=38, total=   8.2s\n",
      "[CV] learning_rate=0.008339092654580042, n_hidden=1, n_neurons=38 ....\n",
      "Epoch 1/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.9047 - val_loss: 1.2874\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.4870 - val_loss: 0.7809\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.4394 - val_loss: 1.8555\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.5436 - val_loss: 18.7096\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.5247 - val_loss: 78.6921\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.8047 - val_loss: 0.4362\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.4359 - val_loss: 0.3913\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.4187 - val_loss: 0.4218\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.4075 - val_loss: 0.4237\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.4053 - val_loss: 0.3656\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3912 - val_loss: 0.4488\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3900 - val_loss: 0.3706\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3853 - val_loss: 0.3624\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3828 - val_loss: 0.4259\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3798 - val_loss: 0.3565\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3770 - val_loss: 0.4250\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3741 - val_loss: 0.3505\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3754 - val_loss: 0.3618\n",
      "Epoch 19/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3701 - val_loss: 0.4284\n",
      "Epoch 20/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3671 - val_loss: 0.3457\n",
      "Epoch 21/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3717 - val_loss: 0.3816\n",
      "Epoch 22/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3663 - val_loss: 0.3408\n",
      "Epoch 23/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3623 - val_loss: 0.3554\n",
      "Epoch 24/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3605 - val_loss: 0.4284\n",
      "Epoch 25/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3601 - val_loss: 0.3360\n",
      "Epoch 26/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3578 - val_loss: 0.3477\n",
      "Epoch 27/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3585 - val_loss: 0.4174\n",
      "Epoch 28/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3566 - val_loss: 0.3349\n",
      "Epoch 29/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3770 - val_loss: 0.3633\n",
      "Epoch 30/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3712 - val_loss: 0.3616\n",
      "Epoch 31/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3517 - val_loss: 0.3402\n",
      "Epoch 32/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3498 - val_loss: 0.5020\n",
      "Epoch 33/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3503 - val_loss: 0.3499\n",
      "Epoch 34/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3469 - val_loss: 0.4519\n",
      "Epoch 35/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3493 - val_loss: 0.3287\n",
      "Epoch 36/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3454 - val_loss: 0.3331\n",
      "Epoch 37/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3439 - val_loss: 0.3294\n",
      "Epoch 38/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3450 - val_loss: 0.3655\n",
      "Epoch 39/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3467 - val_loss: 0.3218\n",
      "Epoch 40/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3438 - val_loss: 0.3303\n",
      "Epoch 41/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3385 - val_loss: 0.5673\n",
      "Epoch 42/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3398 - val_loss: 0.3217\n",
      "Epoch 43/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3429 - val_loss: 0.3217\n",
      "Epoch 44/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3382 - val_loss: 0.4038\n",
      "Epoch 45/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3380 - val_loss: 0.3215\n",
      "Epoch 46/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3421 - val_loss: 0.3412\n",
      "Epoch 47/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3340 - val_loss: 0.3164\n",
      "Epoch 48/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3380 - val_loss: 0.3534\n",
      "Epoch 49/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3435 - val_loss: 0.4147\n",
      "Epoch 50/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3366 - val_loss: 0.3573\n",
      "Epoch 51/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3308 - val_loss: 0.6192\n",
      "Epoch 52/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3347 - val_loss: 0.3145\n",
      "Epoch 53/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3320 - val_loss: 0.3750\n",
      "Epoch 54/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3379 - val_loss: 0.3377\n",
      "Epoch 55/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3496 - val_loss: 0.3759\n",
      "Epoch 56/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3360 - val_loss: 0.3173\n",
      "Epoch 57/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3368 - val_loss: 0.4066\n",
      "Epoch 58/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3314 - val_loss: 0.3240\n",
      "Epoch 59/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3258 - val_loss: 0.3834\n",
      "Epoch 60/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3292 - val_loss: 0.3099\n",
      "Epoch 61/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3337 - val_loss: 0.3125\n",
      "Epoch 62/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3247 - val_loss: 0.3955\n",
      "Epoch 63/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3258 - val_loss: 0.3104\n",
      "Epoch 64/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3248 - val_loss: 0.3076\n",
      "Epoch 65/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3286 - val_loss: 0.3414\n",
      "Epoch 66/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3358 - val_loss: 0.3519\n",
      "Epoch 67/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3301 - val_loss: 0.3122\n",
      "Epoch 68/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3304 - val_loss: 0.3076\n",
      "Epoch 69/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3199 - val_loss: 0.3483\n",
      "Epoch 70/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3200 - val_loss: 0.3423\n",
      "Epoch 71/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3249 - val_loss: 0.3039\n",
      "Epoch 72/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3222 - val_loss: 0.3567\n",
      "Epoch 73/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3286 - val_loss: 0.3128\n",
      "Epoch 74/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3257 - val_loss: 0.3098\n",
      "Epoch 75/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3196 - val_loss: 0.4064\n",
      "Epoch 76/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3401 - val_loss: 0.5664\n",
      "Epoch 77/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3276 - val_loss: 0.3067\n",
      "Epoch 78/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3194 - val_loss: 0.3687\n",
      "Epoch 79/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3168 - val_loss: 0.3029\n",
      "Epoch 80/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3189 - val_loss: 0.3284\n",
      "Epoch 81/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3173 - val_loss: 0.3047\n",
      "Epoch 82/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3164 - val_loss: 0.3430\n",
      "Epoch 83/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3163 - val_loss: 0.3166\n",
      "Epoch 84/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3254 - val_loss: 0.4271\n",
      "Epoch 85/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3398 - val_loss: 0.3018\n",
      "Epoch 86/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3144 - val_loss: 0.3009\n",
      "Epoch 87/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3207 - val_loss: 0.3235\n",
      "Epoch 88/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3148 - val_loss: 0.3020\n",
      "Epoch 89/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3121 - val_loss: 0.3878\n",
      "Epoch 90/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3145 - val_loss: 0.3008\n",
      "Epoch 91/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3169 - val_loss: 0.3182\n",
      "Epoch 92/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3136 - val_loss: 0.3004\n",
      "Epoch 93/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3124 - val_loss: 0.3284\n",
      "Epoch 94/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3119 - val_loss: 0.4480\n",
      "Epoch 95/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3103 - val_loss: 0.3328\n",
      "Epoch 96/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3099 - val_loss: 0.4776\n",
      "Epoch 97/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3454 - val_loss: 0.3339\n",
      "Epoch 98/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3186 - val_loss: 0.3280\n",
      "Epoch 99/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3124 - val_loss: 0.5406\n",
      "Epoch 100/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3114 - val_loss: 0.3038\n",
      "121/121 [==============================] - 0s 1ms/step - loss: 0.3124\n",
      "[CV]  learning_rate=0.008339092654580042, n_hidden=1, n_neurons=38, total=  56.4s\n",
      "[CV] learning_rate=0.00030107783636342726, n_hidden=3, n_neurons=21 ..\n",
      "Epoch 1/100\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 4.0446 - val_loss: 7.0502\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 2.3108 - val_loss: 7.2037\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 1.6259 - val_loss: 5.5884\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 1.3383 - val_loss: 3.7640\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 1.1769 - val_loss: 2.5552\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 1.0669 - val_loss: 2.0914\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.9873 - val_loss: 1.6989\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.9225 - val_loss: 1.4173\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.8695 - val_loss: 1.2066\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.8251 - val_loss: 1.0479\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.7880 - val_loss: 0.9248\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.7571 - val_loss: 0.8264\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.7314 - val_loss: 0.7581\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.7096 - val_loss: 0.7119\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 0.6917 - val_loss: 0.6743\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.6760 - val_loss: 0.6514\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.6629 - val_loss: 0.6371\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 0.6514 - val_loss: 0.6283\n",
      "Epoch 19/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.6412 - val_loss: 0.6229\n",
      "Epoch 20/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.6322 - val_loss: 0.6221\n",
      "Epoch 21/100\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 0.6241 - val_loss: 0.6180\n",
      "Epoch 22/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.6164 - val_loss: 0.6178\n",
      "Epoch 23/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.6092 - val_loss: 0.6150\n",
      "Epoch 24/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.6023 - val_loss: 0.6175\n",
      "Epoch 25/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.5962 - val_loss: 0.6112\n",
      "Epoch 26/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.5900 - val_loss: 0.6049\n",
      "Epoch 27/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.5840 - val_loss: 0.6013\n",
      "Epoch 28/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.5783 - val_loss: 0.5932\n",
      "Epoch 29/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.5726 - val_loss: 0.5873\n",
      "Epoch 30/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.5672 - val_loss: 0.5832\n",
      "Epoch 31/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.5619 - val_loss: 0.5789\n",
      "Epoch 32/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.5568 - val_loss: 0.5713\n",
      "Epoch 33/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.5516 - val_loss: 0.5664\n",
      "Epoch 34/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.5467 - val_loss: 0.5613\n",
      "Epoch 35/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.5418 - val_loss: 0.5537\n",
      "Epoch 36/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.5371 - val_loss: 0.5535\n",
      "Epoch 37/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.5326 - val_loss: 0.5419\n",
      "Epoch 38/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.5281 - val_loss: 0.5418\n",
      "Epoch 39/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.5237 - val_loss: 0.5343\n",
      "Epoch 40/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.5195 - val_loss: 0.5282\n",
      "Epoch 41/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.5153 - val_loss: 0.5252\n",
      "Epoch 42/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.5113 - val_loss: 0.5198\n",
      "Epoch 43/100\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 0.5074 - val_loss: 0.5159\n",
      "Epoch 44/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.5035 - val_loss: 0.5110\n",
      "Epoch 45/100\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 0.4997 - val_loss: 0.5071\n",
      "Epoch 46/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.4959 - val_loss: 0.5049\n",
      "Epoch 47/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.4925 - val_loss: 0.4988\n",
      "Epoch 48/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.4888 - val_loss: 0.4950\n",
      "Epoch 49/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.4853 - val_loss: 0.4902\n",
      "Epoch 50/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.4819 - val_loss: 0.4869\n",
      "Epoch 51/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.4785 - val_loss: 0.4851\n",
      "Epoch 52/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.4754 - val_loss: 0.4779\n",
      "Epoch 53/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.4721 - val_loss: 0.4731\n",
      "Epoch 54/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.4689 - val_loss: 0.4699\n",
      "Epoch 55/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.4658 - val_loss: 0.4657\n",
      "Epoch 56/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.4630 - val_loss: 0.4605\n",
      "Epoch 57/100\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 0.4600 - val_loss: 0.4583\n",
      "Epoch 58/100\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 0.4571 - val_loss: 0.4528\n",
      "Epoch 59/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.4544 - val_loss: 0.4496\n",
      "Epoch 60/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.4516 - val_loss: 0.4473\n",
      "Epoch 61/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.4492 - val_loss: 0.4437\n",
      "Epoch 62/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.4466 - val_loss: 0.4411\n",
      "Epoch 63/100\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 0.4440 - val_loss: 0.4392\n",
      "Epoch 64/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.4418 - val_loss: 0.4340\n",
      "Epoch 65/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.4394 - val_loss: 0.4314\n",
      "Epoch 66/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.4371 - val_loss: 0.4298\n",
      "Epoch 67/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.4349 - val_loss: 0.4278\n",
      "Epoch 68/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.4327 - val_loss: 0.4237\n",
      "Epoch 69/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.4306 - val_loss: 0.4218\n",
      "Epoch 70/100\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 0.4285 - val_loss: 0.4200\n",
      "Epoch 71/100\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 0.4266 - val_loss: 0.4167\n",
      "Epoch 72/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.4246 - val_loss: 0.4143\n",
      "Epoch 73/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.4227 - val_loss: 0.4125\n",
      "Epoch 74/100\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 0.4209 - val_loss: 0.4105\n",
      "Epoch 75/100\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 0.4191 - val_loss: 0.4085\n",
      "Epoch 76/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.4172 - val_loss: 0.4071\n",
      "Epoch 77/100\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 0.4157 - val_loss: 0.4048\n",
      "Epoch 78/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.4140 - val_loss: 0.4032\n",
      "Epoch 79/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.4124 - val_loss: 0.4017\n",
      "Epoch 80/100\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 0.4108 - val_loss: 0.4002\n",
      "Epoch 81/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.4094 - val_loss: 0.3986\n",
      "Epoch 82/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.4079 - val_loss: 0.3973\n",
      "Epoch 83/100\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 0.4065 - val_loss: 0.3959\n",
      "Epoch 84/100\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 0.4051 - val_loss: 0.3949\n",
      "Epoch 85/100\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 0.4037 - val_loss: 0.3931\n",
      "Epoch 86/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.4022 - val_loss: 0.3926\n",
      "Epoch 87/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.4012 - val_loss: 0.3914\n",
      "Epoch 88/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3999 - val_loss: 0.3897\n",
      "Epoch 89/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3987 - val_loss: 0.3888\n",
      "Epoch 90/100\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 0.3975 - val_loss: 0.3880\n",
      "Epoch 91/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3964 - val_loss: 0.3866\n",
      "Epoch 92/100\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 0.3953 - val_loss: 0.3861\n",
      "Epoch 93/100\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 0.3943 - val_loss: 0.3848\n",
      "Epoch 94/100\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 0.3932 - val_loss: 0.3839\n",
      "Epoch 95/100\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 0.3922 - val_loss: 0.3836\n",
      "Epoch 96/100\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 0.3911 - val_loss: 0.3840\n",
      "Epoch 97/100\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 0.3904 - val_loss: 0.3822\n",
      "Epoch 98/100\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 0.3894 - val_loss: 0.3806\n",
      "Epoch 99/100\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 0.3883 - val_loss: 0.3822\n",
      "Epoch 100/100\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 0.3876 - val_loss: 0.3795\n",
      "121/121 [==============================] - 0s 1ms/step - loss: 0.3993\n",
      "[CV]  learning_rate=0.00030107783636342726, n_hidden=3, n_neurons=21, total= 1.0min\n",
      "[CV] learning_rate=0.00030107783636342726, n_hidden=3, n_neurons=21 ..\n",
      "Epoch 1/100\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 5.0701 - val_loss: 2.9725\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 2.1451 - val_loss: 5.9015\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 1.2758 - val_loss: 10.8119\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 1.0903 - val_loss: 11.3108\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 1.0053 - val_loss: 9.9424\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 0.9444 - val_loss: 8.2069\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 0.8977 - val_loss: 6.6004\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 0.8609 - val_loss: 4.8507\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.8311 - val_loss: 3.5263\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.8061 - val_loss: 2.6353\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.7845 - val_loss: 1.9734\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.7657 - val_loss: 1.4481\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.7491 - val_loss: 1.1077\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.7340 - val_loss: 0.8819\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.7201 - val_loss: 0.7221\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.7071 - val_loss: 0.6649\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.6951 - val_loss: 0.6775\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.6837 - val_loss: 0.7491\n",
      "Epoch 19/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.6729 - val_loss: 0.8815\n",
      "Epoch 20/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.6627 - val_loss: 1.0684\n",
      "Epoch 21/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.6528 - val_loss: 1.3065\n",
      "Epoch 22/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.6436 - val_loss: 1.5427\n",
      "Epoch 23/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.6350 - val_loss: 1.8315\n",
      "Epoch 24/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.6268 - val_loss: 2.1426\n",
      "Epoch 25/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.6190 - val_loss: 2.5085\n",
      "Epoch 26/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.6115 - val_loss: 2.8640\n",
      "121/121 [==============================] - 0s 1ms/step - loss: 0.6770\n",
      "[CV]  learning_rate=0.00030107783636342726, n_hidden=3, n_neurons=21, total=  16.2s\n",
      "[CV] learning_rate=0.00030107783636342726, n_hidden=3, n_neurons=21 ..\n",
      "Epoch 1/100\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 4.4059 - val_loss: 3.5308\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 2.5613 - val_loss: 3.0045\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 1.4038 - val_loss: 2.5464\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.9815 - val_loss: 1.8717\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.8396 - val_loss: 1.3067\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.7692 - val_loss: 0.9966\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.7270 - val_loss: 0.8331\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 0.6993 - val_loss: 0.7309\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.6799 - val_loss: 0.6922\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.6652 - val_loss: 0.6623\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.6528 - val_loss: 0.6391\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.6420 - val_loss: 0.6199\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.6325 - val_loss: 0.6066\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.6235 - val_loss: 0.5952\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.6150 - val_loss: 0.5855\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.6070 - val_loss: 0.5761\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.5992 - val_loss: 0.5671\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.5915 - val_loss: 0.5590\n",
      "Epoch 19/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.5841 - val_loss: 0.5515\n",
      "Epoch 20/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.5769 - val_loss: 0.5445\n",
      "Epoch 21/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.5697 - val_loss: 0.5376\n",
      "Epoch 22/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.5627 - val_loss: 0.5308\n",
      "Epoch 23/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.5559 - val_loss: 0.5241\n",
      "Epoch 24/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.5491 - val_loss: 0.5179\n",
      "Epoch 25/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.5425 - val_loss: 0.5114\n",
      "Epoch 26/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.5360 - val_loss: 0.5049\n",
      "Epoch 27/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.5296 - val_loss: 0.4989\n",
      "Epoch 28/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.5234 - val_loss: 0.4930\n",
      "Epoch 29/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.5173 - val_loss: 0.4880\n",
      "Epoch 30/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.5115 - val_loss: 0.4819\n",
      "Epoch 31/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.5058 - val_loss: 0.4768\n",
      "Epoch 32/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.5003 - val_loss: 0.4726\n",
      "Epoch 33/100\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 0.4951 - val_loss: 0.4680\n",
      "Epoch 34/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.4901 - val_loss: 0.4648\n",
      "Epoch 35/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.4854 - val_loss: 0.4611\n",
      "Epoch 36/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.4809 - val_loss: 0.4579\n",
      "Epoch 37/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.4767 - val_loss: 0.4552\n",
      "Epoch 38/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.4727 - val_loss: 0.4523\n",
      "Epoch 39/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.4688 - val_loss: 0.4503\n",
      "Epoch 40/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.4651 - val_loss: 0.4481\n",
      "Epoch 41/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.4616 - val_loss: 0.4479\n",
      "Epoch 42/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.4584 - val_loss: 0.4464\n",
      "Epoch 43/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.4552 - val_loss: 0.4442\n",
      "Epoch 44/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.4524 - val_loss: 0.4442\n",
      "Epoch 45/100\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 0.4495 - val_loss: 0.4436\n",
      "Epoch 46/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.4469 - val_loss: 0.4428\n",
      "Epoch 47/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.4444 - val_loss: 0.4427\n",
      "Epoch 48/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.4420 - val_loss: 0.4431\n",
      "Epoch 49/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.4397 - val_loss: 0.4436\n",
      "Epoch 50/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.4375 - val_loss: 0.4441\n",
      "Epoch 51/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.4353 - val_loss: 0.4441\n",
      "Epoch 52/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.4335 - val_loss: 0.4456\n",
      "Epoch 53/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.4315 - val_loss: 0.4460\n",
      "Epoch 54/100\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 0.4297 - val_loss: 0.4467\n",
      "Epoch 55/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.4278 - val_loss: 0.4484\n",
      "Epoch 56/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.4262 - val_loss: 0.4489\n",
      "Epoch 57/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.4247 - val_loss: 0.4493\n",
      "121/121 [==============================] - 0s 1ms/step - loss: 0.4256\n",
      "[CV]  learning_rate=0.00030107783636342726, n_hidden=3, n_neurons=21, total=  34.7s\n",
      "[CV] learning_rate=0.005153286333701512, n_hidden=1, n_neurons=22 ....\n",
      "Epoch 1/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 1.3002 - val_loss: 38.2652\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.9964 - val_loss: 0.6706\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.5490 - val_loss: 0.5520\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.4986 - val_loss: 0.5090\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.4710 - val_loss: 0.4813\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.4526 - val_loss: 0.4761\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.4406 - val_loss: 0.4565\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.4321 - val_loss: 0.4533\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.4259 - val_loss: 0.4502\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.4210 - val_loss: 0.4389\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.4166 - val_loss: 0.4360\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.4123 - val_loss: 0.4313\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.4086 - val_loss: 0.4253\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.4064 - val_loss: 0.4228\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.4030 - val_loss: 0.4209\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.4011 - val_loss: 0.4192\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3986 - val_loss: 0.4156\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3966 - val_loss: 0.4137\n",
      "Epoch 19/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3949 - val_loss: 0.4128\n",
      "Epoch 20/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3936 - val_loss: 0.4104\n",
      "Epoch 21/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3928 - val_loss: 0.4101\n",
      "Epoch 22/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3908 - val_loss: 0.4070\n",
      "Epoch 23/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3898 - val_loss: 0.4080\n",
      "Epoch 24/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3892 - val_loss: 0.4037\n",
      "Epoch 25/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3878 - val_loss: 0.4030\n",
      "Epoch 26/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3869 - val_loss: 0.4000\n",
      "Epoch 27/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3864 - val_loss: 0.3972\n",
      "Epoch 28/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3851 - val_loss: 0.3974\n",
      "Epoch 29/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3838 - val_loss: 0.3943\n",
      "Epoch 30/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3834 - val_loss: 0.3948\n",
      "Epoch 31/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3818 - val_loss: 0.3922\n",
      "Epoch 32/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3817 - val_loss: 0.3917\n",
      "Epoch 33/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3803 - val_loss: 0.3905\n",
      "Epoch 34/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3798 - val_loss: 0.3893\n",
      "Epoch 35/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3790 - val_loss: 0.3876\n",
      "Epoch 36/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3775 - val_loss: 0.3916\n",
      "Epoch 37/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3769 - val_loss: 0.3831\n",
      "Epoch 38/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3762 - val_loss: 0.3875\n",
      "Epoch 39/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3747 - val_loss: 0.3847\n",
      "Epoch 40/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3750 - val_loss: 0.3846\n",
      "Epoch 41/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3744 - val_loss: 0.3842\n",
      "Epoch 42/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3730 - val_loss: 0.3814\n",
      "Epoch 43/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3735 - val_loss: 0.3808\n",
      "Epoch 44/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3718 - val_loss: 0.3834\n",
      "Epoch 45/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3725 - val_loss: 0.3804\n",
      "Epoch 46/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3704 - val_loss: 0.3824\n",
      "Epoch 47/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3703 - val_loss: 0.3798\n",
      "Epoch 48/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3705 - val_loss: 0.3800\n",
      "Epoch 49/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3686 - val_loss: 0.3783\n",
      "Epoch 50/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3686 - val_loss: 0.3797\n",
      "Epoch 51/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3683 - val_loss: 0.3820\n",
      "Epoch 52/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3675 - val_loss: 0.3765\n",
      "Epoch 53/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3672 - val_loss: 0.3772\n",
      "Epoch 54/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3667 - val_loss: 0.3766\n",
      "Epoch 55/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3656 - val_loss: 0.3773\n",
      "Epoch 56/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3663 - val_loss: 0.3754\n",
      "Epoch 57/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3650 - val_loss: 0.3750\n",
      "Epoch 58/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3645 - val_loss: 0.3750\n",
      "Epoch 59/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3633 - val_loss: 0.3766\n",
      "Epoch 60/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3626 - val_loss: 0.3749\n",
      "Epoch 61/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3629 - val_loss: 0.3764\n",
      "Epoch 62/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3619 - val_loss: 0.3759\n",
      "Epoch 63/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3614 - val_loss: 0.3736\n",
      "Epoch 64/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3607 - val_loss: 0.3750\n",
      "Epoch 65/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3606 - val_loss: 0.3727\n",
      "Epoch 66/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3631 - val_loss: 0.3757\n",
      "Epoch 67/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3598 - val_loss: 0.3733\n",
      "Epoch 68/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3590 - val_loss: 0.3719\n",
      "Epoch 69/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3583 - val_loss: 0.3714\n",
      "Epoch 70/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3589 - val_loss: 0.3698\n",
      "Epoch 71/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3574 - val_loss: 0.3689\n",
      "Epoch 72/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3564 - val_loss: 0.3717\n",
      "Epoch 73/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3572 - val_loss: 0.3699\n",
      "Epoch 74/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3559 - val_loss: 0.3664\n",
      "Epoch 75/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3557 - val_loss: 0.3663\n",
      "Epoch 76/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3545 - val_loss: 0.3689\n",
      "Epoch 77/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3551 - val_loss: 0.3670\n",
      "Epoch 78/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3539 - val_loss: 0.3682\n",
      "Epoch 79/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3534 - val_loss: 0.3647\n",
      "Epoch 80/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3541 - val_loss: 0.3669\n",
      "Epoch 81/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3528 - val_loss: 0.3654\n",
      "Epoch 82/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3529 - val_loss: 0.3639\n",
      "Epoch 83/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3529 - val_loss: 0.3644\n",
      "Epoch 84/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3514 - val_loss: 0.3632\n",
      "Epoch 85/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3511 - val_loss: 0.3619\n",
      "Epoch 86/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3502 - val_loss: 0.3615\n",
      "Epoch 87/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3512 - val_loss: 0.3592\n",
      "Epoch 88/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3499 - val_loss: 0.3633\n",
      "Epoch 89/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3494 - val_loss: 0.3581\n",
      "Epoch 90/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3493 - val_loss: 0.3597\n",
      "Epoch 91/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3489 - val_loss: 0.3582\n",
      "Epoch 92/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3487 - val_loss: 0.3575\n",
      "Epoch 93/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3483 - val_loss: 0.3572\n",
      "Epoch 94/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3485 - val_loss: 0.3582\n",
      "Epoch 95/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3470 - val_loss: 0.3579\n",
      "Epoch 96/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3466 - val_loss: 0.3626\n",
      "Epoch 97/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3466 - val_loss: 0.3564\n",
      "Epoch 98/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3478 - val_loss: 0.3560\n",
      "Epoch 99/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3465 - val_loss: 0.3618\n",
      "Epoch 100/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3458 - val_loss: 0.3552\n",
      "121/121 [==============================] - 0s 1ms/step - loss: 0.3645\n",
      "[CV]  learning_rate=0.005153286333701512, n_hidden=1, n_neurons=22, total=  56.2s\n",
      "[CV] learning_rate=0.005153286333701512, n_hidden=1, n_neurons=22 ....\n",
      "Epoch 1/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 1.2613 - val_loss: 0.6451\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.5785 - val_loss: 0.8942\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.5115 - val_loss: 1.2421\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.4809 - val_loss: 1.2691\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.4597 - val_loss: 0.9915\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.4442 - val_loss: 0.6535\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.4337 - val_loss: 0.5216\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.4234 - val_loss: 0.4130\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.4148 - val_loss: 0.3818\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.4105 - val_loss: 0.4044\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.4043 - val_loss: 0.4355\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3994 - val_loss: 0.4276\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3968 - val_loss: 0.4761\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3953 - val_loss: 0.5445\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3922 - val_loss: 0.5613\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3886 - val_loss: 0.6763\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3861 - val_loss: 0.6692\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3845 - val_loss: 0.7573\n",
      "Epoch 19/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3817 - val_loss: 0.6834\n",
      "121/121 [==============================] - 0s 1ms/step - loss: 0.3963\n",
      "[CV]  learning_rate=0.005153286333701512, n_hidden=1, n_neurons=22, total=  10.9s\n",
      "[CV] learning_rate=0.005153286333701512, n_hidden=1, n_neurons=22 ....\n",
      "Epoch 1/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 1.2040 - val_loss: 71.0120\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 1.0541 - val_loss: 42.2913\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.8661 - val_loss: 1.3112\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.5287 - val_loss: 0.5968\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.4829 - val_loss: 0.4855\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.4636 - val_loss: 0.4448\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.4508 - val_loss: 0.4217\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.4424 - val_loss: 0.4094\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.4358 - val_loss: 0.4025\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.4322 - val_loss: 0.3958\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.4267 - val_loss: 0.3918\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.4230 - val_loss: 0.3892\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.4196 - val_loss: 0.3915\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.4170 - val_loss: 0.3996\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.4136 - val_loss: 0.4076\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.4108 - val_loss: 0.4218\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.4081 - val_loss: 0.4317\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.4068 - val_loss: 0.4354\n",
      "Epoch 19/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.4048 - val_loss: 0.4429\n",
      "Epoch 20/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.4034 - val_loss: 0.4453\n",
      "Epoch 21/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.4021 - val_loss: 0.4460\n",
      "Epoch 22/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.4001 - val_loss: 0.4415\n",
      "121/121 [==============================] - 0s 1ms/step - loss: 0.3947\n",
      "[CV]  learning_rate=0.005153286333701512, n_hidden=1, n_neurons=22, total=  12.5s\n",
      "[CV] learning_rate=0.0003099230412972121, n_hidden=0, n_neurons=49 ...\n",
      "Epoch 1/100\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 7.7197 - val_loss: 43.0907\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 5.5377 - val_loss: 27.4372\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 4.0702 - val_loss: 17.4473\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 3.0672 - val_loss: 11.0914\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 2.3745 - val_loss: 7.0664\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 1.8936 - val_loss: 4.5088\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 1.5544 - val_loss: 2.9277\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 1.3139 - val_loss: 1.9631\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 1.1426 - val_loss: 1.3974\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 1.0191 - val_loss: 1.0599\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.9292 - val_loss: 0.8770\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.8636 - val_loss: 0.7898\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.8150 - val_loss: 0.7557\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.7787 - val_loss: 0.7567\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.7515 - val_loss: 0.7696\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.7308 - val_loss: 0.7947\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.7148 - val_loss: 0.8231\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.7022 - val_loss: 0.8540\n",
      "Epoch 19/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.6920 - val_loss: 0.8834\n",
      "Epoch 20/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.6838 - val_loss: 0.9089\n",
      "Epoch 21/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.6772 - val_loss: 0.9189\n",
      "Epoch 22/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.6713 - val_loss: 0.9382\n",
      "Epoch 23/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.6664 - val_loss: 0.9536\n",
      "121/121 [==============================] - 0s 1ms/step - loss: 0.6673\n",
      "[CV]  learning_rate=0.0003099230412972121, n_hidden=0, n_neurons=49, total=  13.1s\n",
      "[CV] learning_rate=0.0003099230412972121, n_hidden=0, n_neurons=49 ...\n",
      "Epoch 1/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 7.6328 - val_loss: 25.5463\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 5.6957 - val_loss: 23.8232\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 4.3247 - val_loss: 22.6165\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 3.3439 - val_loss: 21.7670\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 2.6347 - val_loss: 21.1673\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 2.1177 - val_loss: 20.7451\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 1.7380 - val_loss: 20.4552\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 1.4573 - val_loss: 20.2628\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 1.2483 - val_loss: 20.1364\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 1.0921 - val_loss: 20.0567\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.9744 - val_loss: 20.0180\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.8854 - val_loss: 20.0099\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.8177 - val_loss: 20.0241\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.7659 - val_loss: 20.0459\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.7259 - val_loss: 20.0826\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.6949 - val_loss: 20.1289\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.6707 - val_loss: 20.1804\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.6517 - val_loss: 20.2376\n",
      "Epoch 19/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.6366 - val_loss: 20.3030\n",
      "Epoch 20/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.6244 - val_loss: 20.3653\n",
      "Epoch 21/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.6146 - val_loss: 20.4378\n",
      "Epoch 22/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.6066 - val_loss: 20.5061\n",
      "121/121 [==============================] - 0s 1ms/step - loss: 1.0973\n",
      "[CV]  learning_rate=0.0003099230412972121, n_hidden=0, n_neurons=49, total=  12.0s\n",
      "[CV] learning_rate=0.0003099230412972121, n_hidden=0, n_neurons=49 ...\n",
      "Epoch 1/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 6.1564 - val_loss: 7.6683\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 4.4886 - val_loss: 4.9412\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 3.3699 - val_loss: 3.3299\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 2.6029 - val_loss: 2.3535\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 2.0673 - val_loss: 1.7864\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 1.6878 - val_loss: 1.4390\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 1.4151 - val_loss: 1.2303\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 1.2174 - val_loss: 1.1115\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 1.0734 - val_loss: 1.0396\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.9677 - val_loss: 0.9896\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.8894 - val_loss: 0.9739\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.8312 - val_loss: 0.9570\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.7875 - val_loss: 0.9426\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.7545 - val_loss: 0.9414\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.7294 - val_loss: 0.9351\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.7100 - val_loss: 0.9457\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.6951 - val_loss: 0.9437\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.6832 - val_loss: 0.9404\n",
      "Epoch 19/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.6737 - val_loss: 0.9554\n",
      "Epoch 20/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.6661 - val_loss: 0.9559\n",
      "Epoch 21/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.6596 - val_loss: 0.9558\n",
      "Epoch 22/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.6543 - val_loss: 0.9576\n",
      "Epoch 23/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.6496 - val_loss: 0.9476\n",
      "Epoch 24/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.6455 - val_loss: 0.9447\n",
      "Epoch 25/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.6419 - val_loss: 0.9405\n",
      "121/121 [==============================] - 0s 1ms/step - loss: 0.6455\n",
      "[CV]  learning_rate=0.0003099230412972121, n_hidden=0, n_neurons=49, total=  13.8s\n",
      "[CV] learning_rate=0.0033625641252688094, n_hidden=2, n_neurons=42 ...\n",
      "Epoch 1/100\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 1.3724 - val_loss: 19.2760\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.8289 - val_loss: 4.6055\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.5845 - val_loss: 0.7004\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.4829 - val_loss: 0.5034\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.4459 - val_loss: 0.4495\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.4238 - val_loss: 0.4262\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.4095 - val_loss: 0.4112\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3994 - val_loss: 0.4155\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3919 - val_loss: 0.4120\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3855 - val_loss: 0.4010\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3805 - val_loss: 0.4074\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3762 - val_loss: 0.3889\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3711 - val_loss: 0.3859\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3705 - val_loss: 0.4045\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3652 - val_loss: 0.3846\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3648 - val_loss: 0.3959\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3611 - val_loss: 0.4089\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3584 - val_loss: 0.3869\n",
      "Epoch 19/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3555 - val_loss: 0.3860\n",
      "Epoch 20/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3541 - val_loss: 0.3805\n",
      "Epoch 21/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3534 - val_loss: 0.3894\n",
      "Epoch 22/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3503 - val_loss: 0.3799\n",
      "Epoch 23/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3493 - val_loss: 0.4104\n",
      "Epoch 24/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3492 - val_loss: 0.3684\n",
      "Epoch 25/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3456 - val_loss: 0.3799\n",
      "Epoch 26/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3455 - val_loss: 0.3619\n",
      "Epoch 27/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3445 - val_loss: 0.3645\n",
      "Epoch 28/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3421 - val_loss: 0.3707\n",
      "Epoch 29/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3407 - val_loss: 0.3731\n",
      "Epoch 30/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3396 - val_loss: 0.3582\n",
      "Epoch 31/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3373 - val_loss: 0.3508\n",
      "Epoch 32/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3376 - val_loss: 0.3451\n",
      "Epoch 33/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3358 - val_loss: 0.3366\n",
      "Epoch 34/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3354 - val_loss: 0.3431\n",
      "Epoch 35/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3337 - val_loss: 0.3285\n",
      "Epoch 36/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3322 - val_loss: 0.3474\n",
      "Epoch 37/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3320 - val_loss: 0.3244\n",
      "Epoch 38/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3304 - val_loss: 0.3484\n",
      "Epoch 39/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3293 - val_loss: 0.3235\n",
      "Epoch 40/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3301 - val_loss: 0.3421\n",
      "Epoch 41/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3288 - val_loss: 0.3287\n",
      "Epoch 42/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3267 - val_loss: 0.3236\n",
      "Epoch 43/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3278 - val_loss: 0.3245\n",
      "Epoch 44/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3251 - val_loss: 0.3410\n",
      "Epoch 45/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3271 - val_loss: 0.3232\n",
      "Epoch 46/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3235 - val_loss: 0.3294\n",
      "Epoch 47/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3236 - val_loss: 0.3361\n",
      "Epoch 48/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3236 - val_loss: 0.3340\n",
      "Epoch 49/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3211 - val_loss: 0.3173\n",
      "Epoch 50/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3211 - val_loss: 0.3412\n",
      "Epoch 51/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3199 - val_loss: 0.3400\n",
      "Epoch 52/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3188 - val_loss: 0.3324\n",
      "Epoch 53/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3189 - val_loss: 0.3272\n",
      "Epoch 54/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3178 - val_loss: 0.3178\n",
      "Epoch 55/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3160 - val_loss: 0.3240\n",
      "Epoch 56/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3161 - val_loss: 0.3195\n",
      "Epoch 57/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3148 - val_loss: 0.3320\n",
      "Epoch 58/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3139 - val_loss: 0.3243\n",
      "Epoch 59/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3134 - val_loss: 0.3100\n",
      "Epoch 60/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3125 - val_loss: 0.3209\n",
      "Epoch 61/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3123 - val_loss: 0.3261\n",
      "Epoch 62/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3112 - val_loss: 0.3365\n",
      "Epoch 63/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3112 - val_loss: 0.3102\n",
      "Epoch 64/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3101 - val_loss: 0.3334\n",
      "Epoch 65/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3092 - val_loss: 0.3252\n",
      "Epoch 66/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3094 - val_loss: 0.3056\n",
      "Epoch 67/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3086 - val_loss: 0.3228\n",
      "Epoch 68/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3078 - val_loss: 0.3250\n",
      "Epoch 69/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3069 - val_loss: 0.3161\n",
      "Epoch 70/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3068 - val_loss: 0.3125\n",
      "Epoch 71/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3058 - val_loss: 0.3200\n",
      "Epoch 72/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3052 - val_loss: 0.3116\n",
      "Epoch 73/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3045 - val_loss: 0.3040\n",
      "Epoch 74/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3044 - val_loss: 0.3228\n",
      "Epoch 75/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3036 - val_loss: 0.3108\n",
      "Epoch 76/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3024 - val_loss: 0.3159\n",
      "Epoch 77/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3030 - val_loss: 0.3015\n",
      "Epoch 78/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3019 - val_loss: 0.3232\n",
      "Epoch 79/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3024 - val_loss: 0.3049\n",
      "Epoch 80/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3004 - val_loss: 0.3124\n",
      "Epoch 81/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.2997 - val_loss: 0.3013\n",
      "Epoch 82/100\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 0.2994 - val_loss: 0.3348\n",
      "Epoch 83/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.2990 - val_loss: 0.2981\n",
      "Epoch 84/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.2982 - val_loss: 0.3495\n",
      "Epoch 85/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.2979 - val_loss: 0.2981\n",
      "Epoch 86/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.2967 - val_loss: 0.3218\n",
      "Epoch 87/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.2968 - val_loss: 0.3029\n",
      "Epoch 88/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.2966 - val_loss: 0.3225\n",
      "Epoch 89/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.2959 - val_loss: 0.2966\n",
      "Epoch 90/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.2948 - val_loss: 0.3189\n",
      "Epoch 91/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.2949 - val_loss: 0.2955\n",
      "Epoch 92/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.2942 - val_loss: 0.3177\n",
      "Epoch 93/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.2940 - val_loss: 0.2973\n",
      "Epoch 94/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.2939 - val_loss: 0.2946\n",
      "Epoch 95/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.2926 - val_loss: 0.3038\n",
      "Epoch 96/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.2913 - val_loss: 0.2969\n",
      "Epoch 97/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.2916 - val_loss: 0.3221\n",
      "Epoch 98/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.2913 - val_loss: 0.2905\n",
      "Epoch 99/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.2906 - val_loss: 0.3527\n",
      "Epoch 100/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.2906 - val_loss: 0.2995\n",
      "121/121 [==============================] - 0s 1ms/step - loss: 0.3203\n",
      "[CV]  learning_rate=0.0033625641252688094, n_hidden=2, n_neurons=42, total=  58.3s\n",
      "[CV] learning_rate=0.0033625641252688094, n_hidden=2, n_neurons=42 ...\n",
      "Epoch 1/100\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 1.2201 - val_loss: 0.8642\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.6048 - val_loss: 0.7994\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.5340 - val_loss: 1.0803\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.4912 - val_loss: 1.1494\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.4606 - val_loss: 0.9498\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.4414 - val_loss: 0.6208\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.4270 - val_loss: 0.4657\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.4148 - val_loss: 0.3888\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.4041 - val_loss: 0.4084\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3972 - val_loss: 0.4312\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3899 - val_loss: 0.5341\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3833 - val_loss: 0.6081\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3796 - val_loss: 0.7209\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3760 - val_loss: 0.8821\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3715 - val_loss: 0.9049\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3680 - val_loss: 0.9792\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3647 - val_loss: 0.9532\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3628 - val_loss: 1.0397\n",
      "121/121 [==============================] - 0s 1ms/step - loss: 0.3909\n",
      "[CV]  learning_rate=0.0033625641252688094, n_hidden=2, n_neurons=42, total=  10.9s\n",
      "[CV] learning_rate=0.0033625641252688094, n_hidden=2, n_neurons=42 ...\n",
      "Epoch 1/100\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 1.1300 - val_loss: 2.2824\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.6910 - val_loss: 2.5063\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.5904 - val_loss: 1.3345\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.5360 - val_loss: 1.8303\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.4879 - val_loss: 1.1690\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.4597 - val_loss: 1.0937\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.4408 - val_loss: 0.5393\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.4235 - val_loss: 0.5528\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.4125 - val_loss: 0.4217\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.4079 - val_loss: 0.3978\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3958 - val_loss: 0.7642\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3934 - val_loss: 0.3953\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3853 - val_loss: 0.3690\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3805 - val_loss: 0.6782\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3793 - val_loss: 0.5137\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3750 - val_loss: 1.5716\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3863 - val_loss: 1.5438\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3814 - val_loss: 2.5256\n",
      "Epoch 19/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3989 - val_loss: 1.2077\n",
      "Epoch 20/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3812 - val_loss: 0.8839\n",
      "Epoch 21/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3663 - val_loss: 0.3408\n",
      "Epoch 22/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3590 - val_loss: 0.3928\n",
      "Epoch 23/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3572 - val_loss: 0.3411\n",
      "Epoch 24/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3572 - val_loss: 0.4823\n",
      "Epoch 25/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3555 - val_loss: 0.3589\n",
      "Epoch 26/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3527 - val_loss: 0.3810\n",
      "Epoch 27/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3504 - val_loss: 0.4593\n",
      "Epoch 28/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3500 - val_loss: 0.3360\n",
      "Epoch 29/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3499 - val_loss: 0.4983\n",
      "Epoch 30/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3480 - val_loss: 0.3747\n",
      "Epoch 31/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3470 - val_loss: 0.4128\n",
      "Epoch 32/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3445 - val_loss: 0.5464\n",
      "Epoch 33/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3461 - val_loss: 0.3828\n",
      "Epoch 34/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3427 - val_loss: 0.5036\n",
      "Epoch 35/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3434 - val_loss: 0.3437\n",
      "Epoch 36/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3398 - val_loss: 0.4820\n",
      "Epoch 37/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3404 - val_loss: 0.3595\n",
      "Epoch 38/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3414 - val_loss: 0.6282\n",
      "121/121 [==============================] - 0s 1ms/step - loss: 0.3388\n",
      "[CV]  learning_rate=0.0033625641252688094, n_hidden=2, n_neurons=42, total=  22.3s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  30 out of  30 | elapsed: 12.0min finished\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-60-d86e0cb35dad>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m rnd_search_cv.fit(X_train, y_train, epochs=100,\n\u001b[1;32m     16\u001b[0m                   \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_valid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_valid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m                   callbacks=[keras.callbacks.EarlyStopping(patience=10)])\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[1;32m    734\u001b[0m             \u001b[0;31m# of the params are estimators as well.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    735\u001b[0m             self.best_estimator_ = clone(clone(base_estimator).set_params(\n\u001b[0;32m--> 736\u001b[0;31m                 **self.best_params_))\n\u001b[0m\u001b[1;32m    737\u001b[0m             \u001b[0mrefit_start_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    738\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/base.py\u001b[0m in \u001b[0;36mclone\u001b[0;34m(estimator, safe)\u001b[0m\n\u001b[1;32m     80\u001b[0m             raise RuntimeError('Cannot clone object %s, as the constructor '\n\u001b[1;32m     81\u001b[0m                                \u001b[0;34m'either does not set or modifies parameter %s'\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 82\u001b[0;31m                                (estimator, name))\n\u001b[0m\u001b[1;32m     83\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mnew_object\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Cannot clone object <tensorflow.python.keras.wrappers.scikit_learn.KerasRegressor object at 0x7ff6dd618588>, as the constructor either does not set or modifies parameter learning_rate"
     ]
    }
   ],
   "source": [
    "from scipy.stats import reciprocal\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "param_distribs = {\n",
    "    \"n_hidden\": [0, 1, 2, 3],\n",
    "    \"n_neurons\": np.arange(1, 100),\n",
    "    \"learning_rate\": reciprocal(3e-4, 3e-2),\n",
    "}\n",
    "\n",
    "rnd_search_cv = RandomizedSearchCV(keras_reg, param_distribs, n_iter=10, cv=3, verbose=2)\n",
    "rnd_search_cv.fit(X_train, y_train, epochs=100,\n",
    "                  validation_data=(X_valid, y_valid),\n",
    "                  callbacks=[keras.callbacks.EarlyStopping(patience=10)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Y accedemos a los resultados del CV como siempre hacemos en sklearn:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Sqry-VbMmtEB"
   },
   "outputs": [],
   "source": [
    "rnd_search_cv.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ghtQ2174mukj"
   },
   "outputs": [],
   "source": [
    "rnd_search_cv.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "t-JpwTtQmv-R"
   },
   "outputs": [],
   "source": [
    "rnd_search_cv.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rNPoFVMRmxQP"
   },
   "outputs": [],
   "source": [
    "rnd_search_cv.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qr3DHzRBmzdd"
   },
   "outputs": [],
   "source": [
    "model = rnd_search_cv.best_estimator_.model\n",
    "model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Otras librerías para Optimización:\n",
    "\n",
    "Hay muchas, que hacen cosas un poco mejores que buscar en una grilla o de forma Random. Acá selecciono tres a dedo (en el Gerón pueden ver más opcionaes también):\n",
    "\n",
    " * [Keras Tuner](https://keras-team.github.io/keras-tuner/): Es parte del ecosistema de Keras, por lo que está optimizado para la versión de tensorflor: tf.keras, y tiene herramientas de visualización útiles.\n",
    " * [Scikit-Optimize](https://scikit-optimize.github.io/): Similar a sklearn para algoritmos de aprendizaje automatizado, es esta librería para algoritmos de optimización. En particular `BayesSearchCV` hace optimización Bayesiana con una interfaz similar al `GridSearchCV`.\n",
    " * [Sklearn-Deap](https://github.com/rsteca/sklearn-deap): Esta pretende imitar al `GridSearchCV` de sklearn para buscar hiperparámetros, pero utilizando algoritmos evolutivos en su lugar."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regularización"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sabemos como hacer más flexible nuestra red: Aumentamos el numero de Layers o de Neuronas por Layer. Pero, cómo la regularizamos para prevenir overfitting?\n",
    "\n",
    "Acá presentamos tres formas:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weight-decay: Regularización $\\ell_2$ ó $\\ell_1$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar a los métodos de regularización Ridge y Lasso para modelos lineales, que añadían un término a la función de costo proporcional a la norma $\\ell_2$ o $\\ell_1$ del vector de pesos, uno puede hacer lo mismo en la red neuronal. \n",
    "\n",
    "Implementar weight decay en una red neuronal en keras, es tan simple como añadir el argumento `kernel_regularizer=` en los layers que queramos regularizar. Los regularizadores $\\ell_2$ o $\\ell_1$ los podemos encontrar en `keras.regularizers.l2()` y  `keras.regularizers.l1()`, o incluso un ElasticNet (combinacion de ambos) en `keras.regularizers.l1_l2()`. Al inicializarlos les deberíamos pasar como argumento el peso que tendrá estos términos (equivalente al `alpha` en sklearn)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#por ejemplo para definir un layer denso ReLy de 100 neuronas y regularización l2 (~Ridge) con alpha=0.01:\n",
    "layer = keras.layers.Dense(100,\n",
    "                          activation='relu',\n",
    "                          kernel_regularizer=keras.regularizers.l2(0.01))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gerón ademas nos da un truco para ahorrarnos escribir esto en cada layer, y cualquier hiperparámetro que se repita: Crear un wrapper a dicho layer con la función de Python `functools.partial`. Esta crea wrappers sencillos a funciones/objetos, que pasan hiperparámetros default."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Por ejemplo el modelo primero del notebook, pero regularizado:\n",
    "from functools import partial\n",
    "\n",
    "RegularizedDense = partial(keras.layers.Dense,\n",
    "                          activation='relu',\n",
    "                          kernel_regularizer=keras.regularizers.l2(0.01))\n",
    "\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=[28, 28]),\n",
    "    RegularizedDense(300),\n",
    "    RegularizedDense(100),\n",
    "    RegularizedDense(10, activation=\"softmax\")\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dropout Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una forma muy popular de hacer regularización fue propuesta en dos papers de 2012 y 2014: En cada paso de entrenamiento, algunas neuronas de la red se \"apagan\" con una probabilidad $p$. Esto solo se hace durante entrenamiento (no durante test o en predicciones). La idea de fondo es que estamos forzando a la red a que no dependa fuertemente de ninguna neurona en particular, sino que sería dar mucha importancia a algún feature en particular, sino que la forzamos a dar resultados aún cuando ciertas neuronas se apagan.\n",
    "\n",
    "En keras, esto ya ha sido implementado como un Layer, así que solamente lo debemos importar. Este se encargará de eliminar en cada training_step algunas neuronas con un determinado `rate`, retropropagar correctamente el error y adaptar los weights correctamente a la hora de evaluación y predicción."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Por ejemplo el modelo primero del notebook, pero con Dropout luego de cada Layer:\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=[28, 28]),\n",
    "    keras.layers.Dropout(rate=0.2), #esto lo que esta apagando son algunos pixels de la imagen.\n",
    "    keras.layers.Dense(300, activation=\"relu\"),\n",
    "    keras.layers.Dropout(rate=0.2),\n",
    "    keras.layers.Dense(100, activation=\"relu\"),\n",
    "    keras.layers.Dropout(rate=0.2),\n",
    "    keras.layers.Dense(10, activation=\"softmax\")\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Early Stopping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si bien a medida que entrenamos, el costo sobre el training set va decreciendo, cuando comenzamos a sobreajustarlo típicamente el costo en el validation set comenzará a subir. Por esto, una forma de regularizar es ir monitoreando la función de costo sobre el conjunto de validación al final de cada época, y si vemos que luego de ciertas épocas la validación no mejora, simplemente dejamos de entrenar. \n",
    "\n",
    "En keras, early stopping es fácilmente implementable con un Callback, como vimos anteriormente en EXTRAS. Para visualizar a lo que nos referimos, en un tiempo corto, entrenemos la misma red que al principio, pero invirtiendo los roles de entrenamiento y validación. Como el conjunto de validación es pequeño, deberíamos ver que enseguida comenzamos a sobreajustarlo.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.backend.clear_session()\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Dense(30, activation=\"relu\", input_shape=[8]),\n",
    "    keras.layers.Dense(30, activation=\"relu\"),\n",
    "    keras.layers.Dense(1)\n",
    "])\n",
    "\n",
    "model.compile(loss=\"mse\", optimizer=keras.optimizers.SGD(lr=1e-3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 100 samples, validate on 3870 samples\n",
      "Epoch 1/50\n",
      "100/100 [==============================] - 1s 11ms/sample - loss: 6.4994 - val_loss: 8.8315\n",
      "Epoch 2/50\n",
      "100/100 [==============================] - 0s 3ms/sample - loss: 6.1169 - val_loss: 7.7324\n",
      "Epoch 3/50\n",
      "100/100 [==============================] - 0s 3ms/sample - loss: 5.6729 - val_loss: 7.0400\n",
      "Epoch 4/50\n",
      "100/100 [==============================] - 0s 3ms/sample - loss: 5.3749 - val_loss: 6.4612\n",
      "Epoch 5/50\n",
      "100/100 [==============================] - 0s 3ms/sample - loss: 5.1087 - val_loss: 5.9315\n",
      "Epoch 6/50\n",
      "100/100 [==============================] - 0s 3ms/sample - loss: 4.8533 - val_loss: 5.4494\n",
      "Epoch 7/50\n",
      "100/100 [==============================] - 0s 3ms/sample - loss: 4.6092 - val_loss: 5.0897\n",
      "Epoch 8/50\n",
      "100/100 [==============================] - 0s 3ms/sample - loss: 4.4117 - val_loss: 4.7139\n",
      "Epoch 9/50\n",
      "100/100 [==============================] - 0s 3ms/sample - loss: 4.1954 - val_loss: 4.3872\n",
      "Epoch 10/50\n",
      "100/100 [==============================] - 0s 3ms/sample - loss: 3.9967 - val_loss: 4.1507\n",
      "Epoch 11/50\n",
      "100/100 [==============================] - 0s 3ms/sample - loss: 3.8319 - val_loss: 3.9437\n",
      "Epoch 12/50\n",
      "100/100 [==============================] - 0s 4ms/sample - loss: 3.6764 - val_loss: 3.7744\n",
      "Epoch 13/50\n",
      "100/100 [==============================] - 0s 5ms/sample - loss: 3.5404 - val_loss: 3.6063\n",
      "Epoch 14/50\n",
      "100/100 [==============================] - 0s 3ms/sample - loss: 3.3954 - val_loss: 3.4690\n",
      "Epoch 15/50\n",
      "100/100 [==============================] - 0s 3ms/sample - loss: 3.2602 - val_loss: 3.3459\n",
      "Epoch 16/50\n",
      "100/100 [==============================] - 0s 3ms/sample - loss: 3.1265 - val_loss: 3.2402\n",
      "Epoch 17/50\n",
      "100/100 [==============================] - 0s 3ms/sample - loss: 2.9921 - val_loss: 3.1602\n",
      "Epoch 18/50\n",
      "100/100 [==============================] - 0s 3ms/sample - loss: 2.8664 - val_loss: 3.1050\n",
      "Epoch 19/50\n",
      "100/100 [==============================] - 0s 3ms/sample - loss: 2.7590 - val_loss: 3.0668\n",
      "Epoch 20/50\n",
      "100/100 [==============================] - 0s 4ms/sample - loss: 2.6647 - val_loss: 3.0438\n",
      "Epoch 21/50\n",
      "100/100 [==============================] - 0s 5ms/sample - loss: 2.5690 - val_loss: 3.0306\n",
      "Epoch 22/50\n",
      "100/100 [==============================] - 0s 3ms/sample - loss: 2.4781 - val_loss: 3.0343\n",
      "Epoch 23/50\n",
      "100/100 [==============================] - 0s 3ms/sample - loss: 2.3884 - val_loss: 3.0461\n",
      "Epoch 24/50\n",
      "100/100 [==============================] - 0s 3ms/sample - loss: 2.3015 - val_loss: 3.0672\n",
      "Epoch 25/50\n",
      "100/100 [==============================] - 0s 3ms/sample - loss: 2.2230 - val_loss: 3.1018\n",
      "Epoch 26/50\n",
      "100/100 [==============================] - 0s 3ms/sample - loss: 2.1534 - val_loss: 3.1407\n",
      "Epoch 27/50\n",
      "100/100 [==============================] - 0s 3ms/sample - loss: 2.0856 - val_loss: 3.1971\n",
      "Epoch 28/50\n",
      "100/100 [==============================] - 0s 3ms/sample - loss: 2.0150 - val_loss: 3.2559\n",
      "Epoch 29/50\n",
      "100/100 [==============================] - 0s 5ms/sample - loss: 1.9595 - val_loss: 3.3362\n",
      "Epoch 30/50\n",
      "100/100 [==============================] - 0s 4ms/sample - loss: 1.8939 - val_loss: 3.4018\n",
      "Epoch 31/50\n",
      "100/100 [==============================] - 0s 3ms/sample - loss: 1.8442 - val_loss: 3.4850\n",
      "Epoch 32/50\n",
      "100/100 [==============================] - 0s 3ms/sample - loss: 1.7905 - val_loss: 3.6057\n",
      "Epoch 33/50\n",
      "100/100 [==============================] - 0s 3ms/sample - loss: 1.7315 - val_loss: 3.7059\n",
      "Epoch 34/50\n",
      "100/100 [==============================] - 0s 3ms/sample - loss: 1.6831 - val_loss: 3.8339\n",
      "Epoch 35/50\n",
      "100/100 [==============================] - 0s 3ms/sample - loss: 1.6241 - val_loss: 3.9209\n",
      "Epoch 36/50\n",
      "100/100 [==============================] - 0s 3ms/sample - loss: 1.5880 - val_loss: 4.0456\n",
      "Epoch 37/50\n",
      "100/100 [==============================] - 0s 3ms/sample - loss: 1.5417 - val_loss: 4.1352\n",
      "Epoch 38/50\n",
      "100/100 [==============================] - 0s 3ms/sample - loss: 1.5091 - val_loss: 4.2446\n",
      "Epoch 39/50\n",
      "100/100 [==============================] - 0s 3ms/sample - loss: 1.4660 - val_loss: 4.4071\n",
      "Epoch 40/50\n",
      "100/100 [==============================] - 0s 3ms/sample - loss: 1.4231 - val_loss: 4.5246\n",
      "Epoch 41/50\n",
      "100/100 [==============================] - 0s 3ms/sample - loss: 1.3911 - val_loss: 4.6071\n",
      "Epoch 42/50\n",
      "100/100 [==============================] - 0s 5ms/sample - loss: 1.3648 - val_loss: 4.7032\n",
      "Epoch 43/50\n",
      "100/100 [==============================] - 0s 4ms/sample - loss: 1.3391 - val_loss: 4.8537\n",
      "Epoch 44/50\n",
      "100/100 [==============================] - 0s 3ms/sample - loss: 1.3053 - val_loss: 4.9605\n",
      "Epoch 45/50\n",
      "100/100 [==============================] - 0s 3ms/sample - loss: 1.2806 - val_loss: 5.0887\n",
      "Epoch 46/50\n",
      "100/100 [==============================] - 0s 3ms/sample - loss: 1.2525 - val_loss: 5.1944\n",
      "Epoch 47/50\n",
      "100/100 [==============================] - 0s 3ms/sample - loss: 1.2296 - val_loss: 5.3260\n",
      "Epoch 48/50\n",
      "100/100 [==============================] - 0s 3ms/sample - loss: 1.2054 - val_loss: 5.5104\n",
      "Epoch 49/50\n",
      "100/100 [==============================] - 0s 3ms/sample - loss: 1.1747 - val_loss: 5.6749\n",
      "Epoch 50/50\n",
      "100/100 [==============================] - 0s 3ms/sample - loss: 1.1468 - val_loss: 5.8744\n"
     ]
    }
   ],
   "source": [
    "#aquí invierto el rol de los datasets\n",
    "history = model.fit(X_train[:100], y_train[:100], epochs=50,\n",
    "                    validation_data=(X_valid, y_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Veamos como dan los plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "pd.DataFrame(history.history).plot(figsize=(8, 5))\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Claramente será mejor simplemente dejar de entrenar en la época 20. Esto lo hacemos con el callback de EarlyStopping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.backend.clear_session()\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Dense(30, activation=\"relu\", input_shape=[8]),\n",
    "    keras.layers.Dense(30, activation=\"relu\"),\n",
    "    keras.layers.Dense(1)\n",
    "])\n",
    "\n",
    "model.compile(loss=\"mse\", optimizer=keras.optimizers.SGD(lr=1e-3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 100 samples, validate on 3870 samples\n",
      "Epoch 1/50\n",
      "100/100 [==============================] - 1s 12ms/sample - loss: 6.4994 - val_loss: 8.8315\n",
      "Epoch 2/50\n",
      "100/100 [==============================] - 0s 3ms/sample - loss: 6.1169 - val_loss: 7.7324\n",
      "Epoch 3/50\n",
      "100/100 [==============================] - 0s 3ms/sample - loss: 5.6729 - val_loss: 7.0400\n",
      "Epoch 4/50\n",
      "100/100 [==============================] - 0s 3ms/sample - loss: 5.3749 - val_loss: 6.4612\n",
      "Epoch 5/50\n",
      "100/100 [==============================] - 0s 3ms/sample - loss: 5.1087 - val_loss: 5.9315\n",
      "Epoch 6/50\n",
      "100/100 [==============================] - 0s 3ms/sample - loss: 4.8533 - val_loss: 5.4494\n",
      "Epoch 7/50\n",
      "100/100 [==============================] - 0s 3ms/sample - loss: 4.6092 - val_loss: 5.0897\n",
      "Epoch 8/50\n",
      "100/100 [==============================] - 0s 3ms/sample - loss: 4.4117 - val_loss: 4.7139\n",
      "Epoch 9/50\n",
      "100/100 [==============================] - 0s 3ms/sample - loss: 4.1954 - val_loss: 4.3872\n",
      "Epoch 10/50\n",
      "100/100 [==============================] - 0s 3ms/sample - loss: 3.9967 - val_loss: 4.1507\n",
      "Epoch 11/50\n",
      "100/100 [==============================] - 0s 4ms/sample - loss: 3.8319 - val_loss: 3.9437\n",
      "Epoch 12/50\n",
      "100/100 [==============================] - 0s 4ms/sample - loss: 3.6764 - val_loss: 3.7744\n",
      "Epoch 13/50\n",
      "100/100 [==============================] - 0s 3ms/sample - loss: 3.5404 - val_loss: 3.6063\n",
      "Epoch 14/50\n",
      "100/100 [==============================] - 0s 3ms/sample - loss: 3.3954 - val_loss: 3.4690\n",
      "Epoch 15/50\n",
      "100/100 [==============================] - 0s 3ms/sample - loss: 3.2602 - val_loss: 3.3459\n",
      "Epoch 16/50\n",
      "100/100 [==============================] - 0s 3ms/sample - loss: 3.1265 - val_loss: 3.2402\n",
      "Epoch 17/50\n",
      "100/100 [==============================] - 0s 3ms/sample - loss: 2.9921 - val_loss: 3.1602\n",
      "Epoch 18/50\n",
      "100/100 [==============================] - 0s 4ms/sample - loss: 2.8664 - val_loss: 3.1050\n",
      "Epoch 19/50\n",
      "100/100 [==============================] - 0s 4ms/sample - loss: 2.7590 - val_loss: 3.0668\n",
      "Epoch 20/50\n",
      "100/100 [==============================] - 0s 4ms/sample - loss: 2.6647 - val_loss: 3.0438\n",
      "Epoch 21/50\n",
      "100/100 [==============================] - 0s 3ms/sample - loss: 2.5690 - val_loss: 3.0306\n",
      "Epoch 22/50\n",
      "100/100 [==============================] - 0s 3ms/sample - loss: 2.4781 - val_loss: 3.0343\n",
      "Epoch 23/50\n",
      "100/100 [==============================] - 0s 3ms/sample - loss: 2.3884 - val_loss: 3.0461\n",
      "Epoch 24/50\n",
      "100/100 [==============================] - 0s 3ms/sample - loss: 2.3015 - val_loss: 3.0672\n",
      "Epoch 25/50\n",
      "100/100 [==============================] - 0s 3ms/sample - loss: 2.2230 - val_loss: 3.1018\n",
      "Epoch 26/50\n",
      "100/100 [==============================] - 0s 3ms/sample - loss: 2.1534 - val_loss: 3.1407\n",
      "Epoch 27/50\n",
      "100/100 [==============================] - 0s 3ms/sample - loss: 2.0856 - val_loss: 3.1971\n",
      "Epoch 28/50\n",
      "100/100 [==============================] - 0s 3ms/sample - loss: 2.0150 - val_loss: 3.2559\n",
      "Epoch 29/50\n",
      "100/100 [==============================] - 0s 5ms/sample - loss: 1.9595 - val_loss: 3.3362\n",
      "Epoch 30/50\n",
      "100/100 [==============================] - 0s 3ms/sample - loss: 1.8939 - val_loss: 3.4018\n",
      "Epoch 31/50\n",
      "100/100 [==============================] - 0s 3ms/sample - loss: 1.8442 - val_loss: 3.4850\n"
     ]
    }
   ],
   "source": [
    "early_stopping_cb = keras.callbacks.EarlyStopping(patience=10,\n",
    "                                                  restore_best_weights=True)\n",
    "history = model.fit(X_train[:100], y_train[:100], epochs=50,\n",
    "                    validation_data=(X_valid, y_valid),\n",
    "                    callbacks=[early_stopping_cb])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "pd.DataFrame(history.history).plot(figsize=(8, 5))\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vemos que paró en la época 31, porque pusimos una paciencia de 10 épocas (espera 10 épocas a ver si vuelve a mejorar, en caso que sea una fluctuación. Sin embargo, como pusimos `restore_best_weights=True`, el modelo fiteado debería corresponder al de la época 21:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.0305901619815088"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X_valid, y_valid, verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Que corresponde a la valid_loss en la época 21."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Keras",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python [conda env:tf]",
   "language": "python",
   "name": "conda-env-tf-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
